{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b27daa-ddee-4c92-96fc-ed6051b2e3c4",
   "metadata": {},
   "source": [
    "# 02 Predicting Armed Conflict Using Protest Data - Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06fec9c-0eaf-4358-882f-207727476beb",
   "metadata": {},
   "source": [
    "<b>Notebook description:</b> This notebook belongs to the replication files for the \"Predicting Armed Conflict Using Protest Data\" article. The notebook . The query sets are defined and executed in the \"predicting_armed_conflict_using_protest_data_01_querysets\" jupyter notebook. Additional transformations are defined in the \"predicting_armed_conflict_using_protest_data_transforms\" .py file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d231cee4-a9b7-48e7-8efa-d039bbd756f6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* [Importing modules](#modules)\n",
    "* [Defining folder structure](#define_folders)\n",
    "* [Specifying global parameters](#specify_parameters)\n",
    "* [Loading query sets](#load_queries)\n",
    "* [Limiting geographical scope](#limit_geo)\n",
    "* [Applying additional transformations](#transforms)\n",
    "* [Training models](#training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a1c06-534b-4cd2-b6e1-007fa76c96c8",
   "metadata": {},
   "source": [
    "## Loading modules<a class=\"anchor\" id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b088405-4587-43c0-abd7-f099aa5dc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import geopandas as gpd\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Views 3\n",
    "import viewser\n",
    "from viewser.operations import fetch\n",
    "from viewser import Queryset, Column\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "import views_dataviz\n",
    "from views_dataviz.map import mapper, utils\n",
    "from views_runs import storage\n",
    "import views_transformation_library.splag4d as spl\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "from ingester3.config import source_db_path\n",
    "\n",
    "# Additional transforms from views2\n",
    "from views_transformation_library.views_2 import ln\n",
    "from views_transformation_library.views_2 import moving_sum\n",
    "from views_transformation_library.views_2 import greater_or_equal\n",
    "\n",
    "# Additional transforms\n",
    "import predicting_armed_conflict_using_protest_data_transforms as transforms\n",
    "import predicting_armed_conflict_using_protest_data_models as organize \n",
    "import predicting_armed_conflict_using_protest_data_utils as utils \n",
    "import predicting_armed_conflict_using_protest_data_eval as evaltools\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import sepplotlib as spl\n",
    "\n",
    "\n",
    "# Other packages\n",
    "import pickle \n",
    "from datetime import datetime\n",
    "import sqlalchemy as sa\n",
    "from patsy import dmatrices\n",
    "import importlib\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48fac7b-06fb-460a-bc71-dad3061ab963",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(transforms)\n",
    "importlib.reload(organize)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(evaltools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c057f70-5cc0-4670-ba01-950d95955dd5",
   "metadata": {},
   "source": [
    "## Defining folder structure<a class=\"anchor\" id=\"define_folders\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01e29b-3b85-49f9-a028-66c985fc7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your username \n",
    "username= 'maxle647'#'yourusername'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56739ca-2392-494f-8817-4281878f5b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path.\n",
    "folder_path = f'/Users/{username}/Dropbox (ViEWS)/Protest article replication' #protest_views3'  # Change path\n",
    "print('Folder path:', folder_path)\n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Set up directory for output\n",
    "folder_path = os.path.join(folder_path, '{sub}')\n",
    "\n",
    "# Define output paths\n",
    "output_paths = {\n",
    "    'descriptives': folder_path.format(sub=f'descriptives'),\n",
    "    'evaluation': folder_path.format(sub=f'evaluation'),\n",
    "    'summary_tables':folder_path.format(sub=f'summary_tables'),\n",
    "    'predictions':folder_path.format(sub=f'predictions'),\n",
    "    'maps':folder_path.format(sub=f'maps'),\n",
    "    'models':folder_path.format(sub=f'models'),\n",
    "    'data':folder_path.format(sub=f'data'),\n",
    "\n",
    "    # Sub folders - ADD more sub_folders\n",
    "    'scores_tables': os.path.join(folder_path.format(sub=f\"evaluation\"), \"scores_tables\"),\n",
    "    'coord_plots': os.path.join(folder_path.format(sub=f\"evaluation\"), \"coord_plots\"),\n",
    "    'bootstrapped': os.path.join(folder_path.format(sub=f\"evaluation\"), \"bootstrapped\"),\n",
    "    'pr_curves': os.path.join(folder_path.format(sub=f\"evaluation\"), \"pr_curves\"),\n",
    "    'features': os.path.join(folder_path.format(sub=f\"evaluation\"), \"features\"),\n",
    "    'bisep': os.path.join(folder_path.format(sub=f\"evaluation\"), \"bisep\"),\n",
    "\n",
    "}\n",
    "\n",
    "# Create new folders if they do not already exist.\n",
    "for k, v in output_paths.items():\n",
    "    if not os.path.isdir(v):\n",
    "        os.makedirs(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749ab78-dcac-49ff-af01-5fd2b8275632",
   "metadata": {},
   "source": [
    "## Specifying global parameters <a class=\"anchor\" id=\"specify_parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c4fc1-7d26-4ec5-a87d-7d7fe1d3aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify whether new data shall be fetched from the database and whether the transformations should be applied.\n",
    "# If set set to False, the variables can be used directly post- fetching and transformations. \n",
    "fetch_from_db = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c52f7-75f0-44b4-bc0d-e3dd05c9ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here the outcome of interest to run the analysis for onset and the new periods.\n",
    "# Possible ids: incidence (main analysis), onset, incidence_np (adjusted period), onset_np (adjusted period)\n",
    "run_outcome = 'incidence' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7d7b8-8555-44f8-8613-b22a0eced89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the outcome variable\n",
    "depvar = 'ged_sb_dummy_dep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bad97a-11f8-4733-a67b-cf4a228800b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whether to retrain the models or start with the analysis directly\n",
    "train = False\n",
    "evaluate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223503ff-742c-46a5-a6b2-92fd6043f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    fetchid_01 = 'protest_paper_0109_2022_incidence_01'\n",
    "    fetchid_02 = 'protest_paper_0109_2022_incidence_02'\n",
    "    fetchid_03_01 = 'protest_paper_0109_2022_incidence_03_01'\n",
    "    fetchid_03_02 = 'protest_paper_0109_2022_incidence_03_02'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7799e-b989-417d-883f-1ad8900388eb",
   "metadata": {},
   "source": [
    "## Loading query sets<a class=\"anchor\" id=\"load_queries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch datasets. Fetching breaks down if not split into multiple parts.\n",
    "if fetch_from_db:\n",
    "    datasets_01 = organize.FetchData(fetchid_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d255371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch datasets. Fetching breaks down if not split into multiple parts.\n",
    "if fetch_from_db:\n",
    "    datasets_02 = organize.FetchData(fetchid_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch datasets. Fetching breaks down if not split into multiple parts.\n",
    "if fetch_from_db:\n",
    "    datasets_03_01 = organize.FetchData(fetchid_03_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch datasets. Fetching breaks down if not split into multiple parts.\n",
    "if fetch_from_db:\n",
    "    datasets_03_02 = organize.FetchData(fetchid_03_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 03_01 and 03_02\n",
    "if fetch_from_db:\n",
    "    datasets_03 = []\n",
    "    datasets_03.append(organize.MergeQueries(datasets_03_01[0]['df'],datasets_03_02[0]['df'],datasets_03_01[0]['Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 03_01 and 03_02\n",
    "if fetch_from_db:\n",
    "    datasets_03.append(organize.MergeQueries(datasets_03_01[1]['df'],datasets_03_02[1]['df'],datasets_03_01[1]['Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf97570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 03_01 and 03_02\n",
    "if fetch_from_db:\n",
    "    datasets_03.append(organize.MergeQueries(datasets_03_01[2]['df'],datasets_03_02[2]['df'],datasets_03_01[2]['Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e32df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append lists of dictonaries. \n",
    "if fetch_from_db:\n",
    "    datasets = datasets_01 + datasets_02 + datasets_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54802da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check integrity of data sets.\n",
    "if fetch_from_db:\n",
    "    for df in datasets:\n",
    "        organize.data_integrity_check(df, depvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae996aca",
   "metadata": {},
   "source": [
    "## Limiting geographical and temporal scope<a class=\"anchor\" id=\"limit_geo\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f7c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pg ids for africa only.\n",
    "if fetch_from_db:\n",
    "    df_pg = organize.fetch_africa_ids()\n",
    "\n",
    "    for df in datasets:\n",
    "        organize.crop_africa(df,df_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter month ids.\n",
    "if fetch_from_db:\n",
    "    for df in datasets:\n",
    "        print(df['Name'])\n",
    "        df['df'] = organize.crop_months(df['df'],200,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba170543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex dataset.\n",
    "if fetch_from_db:\n",
    "    for df in datasets:\n",
    "        print(df['Name'])\n",
    "        df['df'] = organize.reindex_df(df['df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a32984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Nas.\n",
    "if fetch_from_db:\n",
    "    for df in datasets:\n",
    "        print(df['df'].isna().any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea0403-6344-4b80-bd32-63c9c6f735a8",
   "metadata": {},
   "source": [
    "## Applying transformations<a class=\"anchor\" id=\"transforms\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78706e2-bd24-486a-b7a7-ac636e5a6bff",
   "metadata": {},
   "source": [
    "### Full economic development, country level and subnational level\n",
    "\n",
    "Apply transformation to all the models that include the full economic development features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ea765-9f47-4278-82ce-c96f3958ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    input_var = 'pgd_gcp_mer'\n",
    "    transf_var = 'pgd_gcp_mer_pc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b3050-1df5-4f49-8e78-10b14cab7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Apply transformation only to first df.\n",
    "\n",
    "    applied_to_df = []\n",
    "    apply_to_dfs = []\n",
    "\n",
    "    first = True\n",
    "    for df in datasets:\n",
    "        if 'econ_full' in df['Name']:\n",
    "            if first:\n",
    "                print(df['Name'])\n",
    "                applied_to_df.append(df)\n",
    "\n",
    "                # Normalise.\n",
    "                df['df'][f'{transf_var}'] = transforms.divide_by_pop(df['df'],f'{input_var}',10000)\n",
    "\n",
    "                # Nat log.\n",
    "                df['df'][f'ln_{transf_var}'] = ln(df['df'][f'{transf_var}'])\n",
    "\n",
    "                if df['df'][f'ln_{transf_var}'].equals(df['df'][f'{transf_var}']) == True:\n",
    "                    print('Warning, check log transformation')\n",
    "\n",
    "                # Drop other variables from df. Keep population variable for the moment as it will be needed for additional transformations. \n",
    "                print('n cols before:', len(df['df'].columns))\n",
    "                df['df'] = df['df'].drop(labels=[input_var,transf_var], axis=1)\n",
    "                print('n cols after:', len(df['df'].columns))\n",
    "\n",
    "                first = False\n",
    "            else:\n",
    "                print(df['Name'])\n",
    "                apply_to_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762644b-c97c-438a-9ae8-7861ce656fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Concat to other dfs.\n",
    "    for df in apply_to_dfs:\n",
    "        print(df['Name'])\n",
    "        df['df'] = pd.concat([df['df'],applied_to_df[0]['df']['ln_pgd_gcp_mer_pc']],axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56f534-5e07-49c6-a05f-cc4028c94b67",
   "metadata": {},
   "source": [
    "### Protest models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca7dcf-2a00-40b8-8abf-25f8dee7f3df",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### All protest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421f3bc-d93b-48dc-8d9f-afc5591d4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    pr_naive_cat = ['']\n",
    "    pr_categories = ['ri','in','ex','pe']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc33ae-cf14-4dd0-8cf3-221e857a05e0",
   "metadata": {},
   "source": [
    "##### Applying transforms to naive and dynamic local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b86862-f099-4f5f-b1be-40c081ddd24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Ln_protestcat_pop_tlag0\n",
    "\n",
    "    applied_to_df = []\n",
    "    apply_to_dfs = []\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for df in datasets:\n",
    "        if 'pr' in df['Name']:\n",
    "\n",
    "            if 'naive_bl' in df['Name']: \n",
    "                #print(df['Name'])\n",
    "\n",
    "                for pr in pr_naive_cat:\n",
    "\n",
    "                    # Normalise.\n",
    "                    print('normalise variable')\n",
    "                    print(df['Name'])\n",
    "                    df['df'][f'acled_pr{pr}_pop'] = transforms.divide_by_pop(df['df'],f'acled_pr{pr}_count',10000)\n",
    "\n",
    "                    # Nat log.\n",
    "                    df['df'][f'ln_acled_pr{pr}_pop_tlag0'] = ln(df['df'][f'acled_pr{pr}_pop'])\n",
    "\n",
    "                    if df['df'][f'ln_acled_pr{pr}_pop_tlag0'].equals(df['df'][f'acled_pr{pr}_pop']) == True:\n",
    "                        print('Warning, check log transformation')\n",
    "            else:\n",
    "                if first:\n",
    "                    applied_to_df.append(df)\n",
    "\n",
    "                    for pr in pr_categories:\n",
    "\n",
    "                        # Normalise.\n",
    "                        print('normalise variable')\n",
    "                        print(df['Name'])\n",
    "                        df['df'][f'acled_pr{pr}_pop'] = transforms.divide_by_pop(df['df'],f'acled_pr{pr}_count',10000)\n",
    "\n",
    "                        # Nat log.\n",
    "                        df['df'][f'ln_acled_pr{pr}_pop_tlag0'] = ln(df['df'][f'acled_pr{pr}_pop'])\n",
    "\n",
    "                        if df['df'][f'ln_acled_pr{pr}_pop_tlag0'].equals(df['df'][f'acled_pr{pr}_pop']) == True:\n",
    "                            print('Warning, check log transformation')\n",
    "\n",
    "                    first = False\n",
    "\n",
    "                else:\n",
    "                    print(df['Name'])\n",
    "                    apply_to_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfb437-57b6-4495-91ae-a68c51a1a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Cumulative sum over three months (normalised) + nat. log.\n",
    "    applied_to_df = []\n",
    "    apply_to_dfs = []\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for df in datasets:\n",
    "        if 'pr' in df['Name']:\n",
    "            print(df['Name'])\n",
    "\n",
    "            if 'naive_bl' in df['Name']: \n",
    "\n",
    "                for pr in pr_naive_cat:\n",
    "\n",
    "                    # Cumulative sum\n",
    "                    print('compute cumulative sum')\n",
    "                    df['df'][f'cumsum_3_acled_pr{pr}_count'] = transforms.moving_sum(s=df['df'][f'acled_pr{pr}_count'], time=3)\n",
    "                    print('Negative values after applying cumulative sum:', df['df'][f'cumsum_3_acled_pr{pr}_count'].any()<0 == True)\n",
    "\n",
    "                    # Normalise.\n",
    "                    print('normalise variable')\n",
    "                    df['df'][f'cumsum_3_acled_pr{pr}_pop'] = transforms.divide_by_pop(df['df'],f'cumsum_3_acled_pr{pr}_count',10000)\n",
    "\n",
    "                    # Nat log.\n",
    "                    df['df'][f'ln_cumsum_3_acled_pr{pr}_pop'] = ln(df['df'][f'cumsum_3_acled_pr{pr}_pop'])\n",
    "                    if df['df'][f'ln_cumsum_3_acled_pr{pr}_pop'].equals(df['df'][f'cumsum_3_acled_pr{pr}_pop']) == True:\n",
    "                        print('Warning, check log transformation') \n",
    "                    else:\n",
    "                        print('Ok')\n",
    "            else:\n",
    "                if first:\n",
    "                    applied_to_df.append(df)\n",
    "\n",
    "                    for pr in pr_categories:\n",
    "\n",
    "                        # Cumulative sum\n",
    "                        print('compute cumulative sum')\n",
    "                        df['df'][f'cumsum_3_acled_pr{pr}_count'] = transforms.moving_sum(s=df['df'][f'acled_pr{pr}_count'], time=3)\n",
    "                        print('Negative values after applying cumulative sum:', df['df'][f'cumsum_3_acled_pr{pr}_count'].any()<0 == True)\n",
    "\n",
    "                        # Normalise.\n",
    "                        print('normalise variable')\n",
    "                        df['df'][f'cumsum_3_acled_pr{pr}_pop'] = transforms.divide_by_pop(df['df'],f'cumsum_3_acled_pr{pr}_count',10000)\n",
    "\n",
    "                        # Nat log.\n",
    "                        df['df'][f'ln_cumsum_3_acled_pr{pr}_pop'] = ln(df['df'][f'cumsum_3_acled_pr{pr}_pop'])\n",
    "                        if df['df'][f'ln_cumsum_3_acled_pr{pr}_pop'].equals(df['df'][f'cumsum_3_acled_pr{pr}_pop']) == True:\n",
    "                            print('Warning, check log transformation')\n",
    "                        else:\n",
    "                            print('Ok')\n",
    "\n",
    "                    first = False\n",
    "\n",
    "                else:\n",
    "                    print(df['Name'])\n",
    "                    apply_to_dfs.append(df)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96724a-546b-4770-a7c7-ab5eba0a81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Splag, normalised by population\n",
    "    applied_to_df = []\n",
    "    apply_to_dfs = []\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for df in datasets:\n",
    "        if 'pr' in df['Name']:\n",
    "            print(df['Name'])\n",
    "\n",
    "            if 'naive_bl' in df['Name']: \n",
    "\n",
    "                for pr in pr_naive_cat:\n",
    "\n",
    "                    # Splag.\n",
    "                    print('take splag of normalised variable')\n",
    "                    df['df'][f'splag_1_2_acled_pr{pr}_pop'] = spl.get_splag4d(pd.DataFrame(df['df'][f'acled_pr{pr}_pop']),True,1,2,0,0).fillna(0)\n",
    "\n",
    "                    # Nat log.\n",
    "                    df['df'][f'ln_splag_1_2_acled_pr{pr}_pop_tlag0'] = ln(df['df'][f'splag_1_2_acled_pr{pr}_pop'])\n",
    "                    if df['df'][f'ln_splag_1_2_acled_pr{pr}_pop_tlag0'].equals(df['df'][f'splag_1_2_acled_pr{pr}_pop']) == True:\n",
    "                        print('Warning, check log transformation') \n",
    "                    else:\n",
    "                        print('Ok')\n",
    "\n",
    "            else:\n",
    "                if first:\n",
    "                    applied_to_df.append(df)\n",
    "\n",
    "                    for pr in pr_categories:\n",
    "\n",
    "                        # Splag.\n",
    "                        print('take splag of normalised variable')\n",
    "                        df['df'][f'splag_1_2_acled_pr{pr}_pop'] = spl.get_splag4d(pd.DataFrame(df['df'][f'acled_pr{pr}_pop']),True,1,2,0,0).fillna(0)\n",
    "\n",
    "                        # Nat log.\n",
    "                        df['df'][f'ln_splag_1_2_acled_pr{pr}_pop_tlag0'] = ln(df['df'][f'splag_1_2_acled_pr{pr}_pop'])\n",
    "                        if df['df'][f'ln_splag_1_2_acled_pr{pr}_pop_tlag0'].equals(df['df'][f'splag_1_2_acled_pr{pr}_pop']) == True:\n",
    "                            print('Warning, check log transformation') \n",
    "                        else:\n",
    "                            print('Ok')\n",
    "\n",
    "                    first = False\n",
    "\n",
    "                else:\n",
    "                    print(df['Name'])\n",
    "                    apply_to_dfs.append(df)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98458c93-725c-420a-832e-638fecf54f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db: \n",
    "    # Cumulative sum of splag, normalised by population\n",
    "    applied_to_df = []\n",
    "    apply_to_dfs = []\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for df in datasets:\n",
    "        if 'pr' in df['Name']:\n",
    "            print(df['Name'])\n",
    "\n",
    "            if 'naive_bl' in df['Name']: \n",
    "\n",
    "                for pr in pr_naive_cat:\n",
    "\n",
    "                    # Splag.\n",
    "                    print('take splag of normalised variable')\n",
    "                    df['df'][f'splag_1_2_cumsum_3_acled_pr{pr}_pop'] = spl.get_splag4d(pd.DataFrame(df['df'][f'cumsum_3_acled_pr{pr}_pop']),True,1,2,0,0).fillna(0)\n",
    "\n",
    "                    # Nat log.\n",
    "                    df['df'][f'ln_splag_1_2_cumsum_3_acled_pr{pr}_pop'] = ln(df['df'][f'splag_1_2_cumsum_3_acled_pr{pr}_pop'])\n",
    "                    if df['df'][f'ln_splag_1_2_cumsum_3_acled_pr{pr}_pop'].equals(df['df'][f'splag_1_2_cumsum_3_acled_pr{pr}_pop']) == True:\n",
    "                        print('Warning, check log transformation') \n",
    "                    else:\n",
    "                        print('Ok')\n",
    "\n",
    "            else:\n",
    "                if first:\n",
    "                    applied_to_df.append(df)\n",
    "\n",
    "                    for pr in pr_categories:\n",
    "\n",
    "                        # Splag.\n",
    "                        print('take splag of normalised variable')\n",
    "                        df['df'][f'splag_1_2_cumsum_3_acled_pr{pr}_pop'] = spl.get_splag4d(pd.DataFrame(df['df'][f'cumsum_3_acled_pr{pr}_pop']),True,1,2,0,0).fillna(0)\n",
    "\n",
    "                        # Nat log.\n",
    "                        df['df'][f'ln_splag_1_2_cumsum_3_acled_pr{pr}_pop'] = ln(df['df'][f'splag_1_2_cumsum_3_acled_pr{pr}_pop'])\n",
    "                        if df['df'][f'ln_splag_1_2_cumsum_3_acled_pr{pr}_pop'].equals(df['df'][f'splag_1_2_cumsum_3_acled_pr{pr}_pop']) == True:\n",
    "                            print('Warning, check log transformation') \n",
    "                        else:\n",
    "                            print('Ok')\n",
    "\n",
    "                    first = False\n",
    "\n",
    "                else:\n",
    "                    print(df['Name'])\n",
    "                    apply_to_dfs.append(df)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26ba39-7930-47ca-936a-f0d336170a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Fetch gdf\n",
    "    gdf = organize.fetch_gdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc90393-d8c9-4fca-999c-6541df0ac272",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Make sure indices are equal\n",
    "    idx1 = gdf.index \n",
    "    idx2 = datasets[1]['df'].index\n",
    "    idx1.equals(idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a59eb5-62f6-430f-8509-3a9c3d2710df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Concat to dataframes\n",
    "    for df in datasets:\n",
    "        print(df['Name'])\n",
    "        df['df'] = pd.concat([df['df'],gdf],axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ad895-5afe-4a62-a56c-af7edfe69ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Minimum distance to closest protset event over three months.\n",
    "    applied_to_df = []\n",
    "    apply_to_dfs = []\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for df in datasets:\n",
    "        if 'pr' in df['Name']:\n",
    "            print(df['Name'])\n",
    "\n",
    "            if 'naive_bl' in df['Name']: \n",
    "\n",
    "                for pr in pr_naive_cat:\n",
    "\n",
    "                    # Get dummy variable.\n",
    "                    print('get dummy')\n",
    "                    df['df'][f'acled_pr{pr}_dummy'] = greater_or_equal(df['df'][f'acled_pr{pr}_count'],1)\n",
    "\n",
    "                    # Compute distance.\n",
    "                    print('compute distance')\n",
    "                    df['df'][f'dist_acled_pr{pr}_dummy'] = transforms.distance_to_event(df=df['df'],col=f'acled_pr{pr}_dummy',k=1,fill_value=99)\n",
    "\n",
    "                    # Get minimum distance over three months.\n",
    "                    print('get minimum distance')\n",
    "                    df['df'][f'min_dist_acled_pr{pr}_dummy'] = transforms.moving_min(s=df['df'][f'dist_acled_pr{pr}_dummy'],t=3)\n",
    "\n",
    "                    # Nat log.\n",
    "                    df['df'][f'ln_min_dist_3_acled_pr{pr}'] = ln(df['df'][f'min_dist_acled_pr{pr}_dummy'])\n",
    "\n",
    "                    if df['df'][f'ln_min_dist_3_acled_pr{pr}'].equals(df['df'][f'min_dist_acled_pr{pr}_dummy']) == True:\n",
    "                        print('Warning, check log transformation')\n",
    "                    else:\n",
    "                        print('Ok')\n",
    "\n",
    "            else:\n",
    "                if first:\n",
    "                    applied_to_df.append(df)\n",
    "\n",
    "                    for pr in pr_categories:\n",
    "\n",
    "                        # Get dummy variable.\n",
    "                        print('get dummy')\n",
    "                        df['df'][f'acled_pr{pr}_dummy'] = greater_or_equal(df['df'][f'acled_pr{pr}_count'],1)\n",
    "\n",
    "                        # Compute distance.\n",
    "                        print('compute distance')\n",
    "                        df['df'][f'dist_acled_pr{pr}_dummy'] = transforms.distance_to_event(df=df['df'],col=f'acled_pr{pr}_dummy',k=1,fill_value=99)\n",
    "\n",
    "                        # Get minimum distance over three months.\n",
    "                        print('get minimum distance')\n",
    "                        df['df'][f'min_dist_acled_pr{pr}_dummy'] = transforms.moving_min(s=df['df'][f'dist_acled_pr{pr}_dummy'],t=3)\n",
    "\n",
    "                        # Nat log.\n",
    "                        df['df'][f'ln_min_dist_3_acled_pr{pr}'] = ln(df['df'][f'min_dist_acled_pr{pr}_dummy'])\n",
    "\n",
    "                        if df['df'][f'ln_min_dist_3_acled_pr{pr}'].equals(df['df'][f'min_dist_acled_pr{pr}_dummy']) == True:\n",
    "                            print('Warning, check log transformation')\n",
    "                        else:\n",
    "                            print('Ok')\n",
    "\n",
    "                    first = False\n",
    "\n",
    "                else:\n",
    "                    print(df['Name'])\n",
    "                    apply_to_dfs.append(df)\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eae3db-d18e-41de-8e70-804f51d9e4a6",
   "metadata": {},
   "source": [
    "##### Adding transforms to remaining protest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28abb4-2636-450c-8967-5bf4410420d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    feats_concat_pr = []\n",
    "    for pr in pr_categories:\n",
    "        feats_concat_pr.append(f'decay_ts_6_acled_pr{pr}_dummy')\n",
    "        feats_concat_pr.append(f'ln_acled_pr{pr}_pop_tlag0')\n",
    "        feats_concat_pr.append(f'ln_cumsum_3_acled_pr{pr}_pop')\n",
    "        feats_concat_pr.append(f'decay_ts_6_splag_1_2_acled_pr{pr}_dummy')\n",
    "        feats_concat_pr.append(f'ln_splag_1_2_acled_pr{pr}_pop_tlag0')\n",
    "        feats_concat_pr.append(f'ln_splag_1_2_cumsum_3_acled_pr{pr}_pop')\n",
    "        feats_concat_pr.append(f'ln_min_dist_3_acled_pr{pr}')\n",
    "    feats_concat_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0470317-80d1-4e3c-ba80-24436bbeeee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Concat with other dfs.\n",
    "    for df in apply_to_dfs:\n",
    "        print(df['Name'])\n",
    "        df['df'] = pd.concat([df['df'],applied_to_df[0]['df'][feats_concat_pr]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0da2d-33ef-44f2-9dff-8edb78ac2c95",
   "metadata": {},
   "source": [
    "#### All models besides local dynamic and naive protest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb03e1-57f2-4e19-acd8-988154e371dd",
   "metadata": {},
   "source": [
    "##### Applying transforms to national models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da101ffd-bd58-416a-88a8-31baba10839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Ln_protestcat_pop_cm_tlag0\n",
    "    applied_to_df = []\n",
    "    apply_to_dfs = []\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for df in datasets:\n",
    "        if 'pr' in df['Name']:\n",
    "            if df['Name'] not in ['pr_naive_bl','pr_dynamic_loc_bl']:\n",
    "                if first:\n",
    "                    applied_to_df.append(df)\n",
    "                    print(df['Name'])\n",
    "\n",
    "                    for pr in pr_categories:\n",
    "                        print(pr)\n",
    "\n",
    "                        # Normalise.\n",
    "                        print('normalise variable')\n",
    "                        df['df'][f'acled_pr{pr}_pop_cm'] = transforms.divide_by_pop_cm(df['df'],f'acled_pr{pr}_count',10000)\n",
    "\n",
    "                        # Nat log.\n",
    "                        df['df'][f'ln_acled_pr{pr}_pop_cm_tlag0'] = ln(df['df'][f'acled_pr{pr}_pop_cm'])\n",
    "\n",
    "                        if df['df'][f'ln_acled_pr{pr}_pop_cm_tlag0'].equals(df['df'][f'acled_pr{pr}_pop_cm']) == True:\n",
    "                            print('Warning, check log transformation')\n",
    "                        else:\n",
    "                            print('OK')\n",
    "\n",
    "                    first = False\n",
    "\n",
    "                else:\n",
    "                    apply_to_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37f032-39c9-44c4-a7fc-055ab9becb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    applied_to_df = []\n",
    "    apply_to_dfs = []\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for df in datasets:\n",
    "        if 'pr' in df['Name']:\n",
    "            if df['Name'] not in ['pr_naive_bl','pr_dynamic_loc_bl']:\n",
    "                if first:\n",
    "                    applied_to_df.append(df)\n",
    "                    print(df['Name'])\n",
    "\n",
    "                    for pr in pr_categories:\n",
    "                        print(pr)\n",
    "\n",
    "                        # Cumulative sum\n",
    "                        print('compute cumulative sum')\n",
    "                        df['df'][f'cumsum_3_acled_pr{pr}_count_cm'] = transforms.moving_sum(s=df['df'][f'acled_pr{pr}_count_cm'], time=3)\n",
    "                        print('Negative values after applying cumulative sum:', df['df'][f'cumsum_3_acled_pr{pr}_count_cm'].any()<0 == True)\n",
    "\n",
    "                        # Normalise.\n",
    "                        print('normalise variable')\n",
    "                        df['df'][f'cumsum_3_acled_pr{pr}_pop_cm'] = transforms.divide_by_pop_cm(df['df'],f'cumsum_3_acled_pr{pr}_count_cm',10000)\n",
    "\n",
    "                        # Nat log.\n",
    "                        df['df'][f'ln_cumsum_3_acled_pr{pr}_pop_cm'] = ln(df['df'][f'cumsum_3_acled_pr{pr}_pop_cm'])\n",
    "                        if df['df'][f'ln_cumsum_3_acled_pr{pr}_pop_cm'].equals(df['df'][f'cumsum_3_acled_pr{pr}_pop_cm']) == True:\n",
    "                            print('Warning, check log transformation') \n",
    "                        else:\n",
    "                                print('OK')\n",
    "\n",
    "                    first = False\n",
    "\n",
    "                else:\n",
    "                    apply_to_dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8f40f-0726-4915-ab25-826add73f5d6",
   "metadata": {},
   "source": [
    "##### Adding transforms to remaining protest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec09517-8933-464a-bdb5-2a956ed7c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    feats_concat_pr = []\n",
    "    for pr in pr_categories:\n",
    "        feats_concat_pr.append(f'decay_ts_6_acled_pr{pr}_dummy_cm')\n",
    "        feats_concat_pr.append(f'ln_acled_pr{pr}_pop_cm_tlag0')\n",
    "        feats_concat_pr.append(f'ln_cumsum_3_acled_pr{pr}_pop_cm')\n",
    "    feats_concat_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4e29c-cef8-445d-aa9d-f3befa07c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Concat with other dfs.\n",
    "    for df in apply_to_dfs:\n",
    "        print(df['Name'])\n",
    "        df['df'] = pd.concat([df['df'],applied_to_df[0]['df'][feats_concat_pr]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a5135-107f-4067-9389-8c48305f0a57",
   "metadata": {},
   "source": [
    "### Political instiutions models (IV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd37365-b619-4bcf-a9f7-fc92404d875f",
   "metadata": {},
   "source": [
    "As discussed in the theoretical section,\n",
    "protests are more common and widely accepted as political behavior in democracies than\n",
    "in non-democracies. To capture this, we include the residuals from a fitted negative\n",
    "binomial regression model with the count of protest with excessive violence events as the\n",
    "dependent variables as a proxy for an unexpected amount of protests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7bdc0c-0503-41d4-8521-cfc7ff585789",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- estimate a model with peacful protests as DV \n",
    "- save residuals from model, i.e. the remaining variation in peacful protests that can not be explaiend by the variables included in our regression\n",
    "- estimate a second model with protests with excessive violence as DV\n",
    "- save fitted values from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894934b-e0f7-4db0-9413-3c3e21b207c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    applied_to_df = []\n",
    "    apply_to_dfs = []\n",
    "\n",
    "    first = True\n",
    "\n",
    "    for df in datasets:\n",
    "        if 'devi' in df['Name']:\n",
    "            if first:\n",
    "                applied_to_df.append(df)\n",
    "                first = False\n",
    "\n",
    "            else:\n",
    "                apply_to_dfs.append(df)\n",
    "\n",
    "    print(applied_to_df[0]['Name'])\n",
    "\n",
    "    for dfname in apply_to_dfs:\n",
    "        print(dfname['Name'])\n",
    "\n",
    "    df_inst_devi_bl_raw = applied_to_df[0]['df']\n",
    "    df_inst_devi_bl_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a560096-420a-4536-bba7-c0fc3c47c695",
   "metadata": {},
   "source": [
    "#### Estimating peaceful protests - extracting residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b87cb-497d-42d8-9c1c-a3b86a2c7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Set up regression expressions\n",
    "    expr_peace = \"\"\"acled_prpe_count ~ \n",
    "    vdem_v2x_polyarchy_tlag12 + \n",
    "    vdem_v2x_civlib_tlag12 + \n",
    "    ln_acled_prpe_count_tlag1 + \n",
    "    ln_acled_prex_count_tlag1 +\n",
    "    ln_geb_sb_best_tlag1 +\n",
    "    ln_geb_os_best_tlag1 +\n",
    "    ln_splag_1_1_ged_sb_best_tlag1 +\n",
    "    ln_splag_1_1_ged_os_best_tlag1 +\n",
    "    ln_splag_1_2_acled_prpe_count_tlag1 +\n",
    "    ln_splag_1_2_acled_prex_count_tlag1 +\n",
    "    pgd_pop_gpw_sum \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb5b60-ead8-4dcf-9add-d6e578dc625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Fit poission.\n",
    "    y_deviant, X_deviant = dmatrices(expr_peace, df_inst_devi_bl_raw.loc[205:444], return_type='dataframe')\n",
    "    poi_results = sm.GLM(y_deviant, X_deviant, family=sm.families.Poisson()).fit()\n",
    "    print(poi_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cad60f-0731-4c73-9d6f-cd36cbf6c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Check for overdispersion\n",
    "\n",
    "    def ct_response(row):\n",
    "        \"Calculate response observation for Cameron-Trivedi dispersion test\"\n",
    "        y = row['acled_prpe_count']\n",
    "        m = row['bev_mu']\n",
    "        return ((y - m)**2 - y) / m\n",
    "\n",
    "    ct_data = df_inst_devi_bl_raw.loc[205:444].copy()\n",
    "    ct_data['bev_mu'] = poi_results.mu\n",
    "    ct_data['ct_resp'] = ct_data.apply(ct_response, axis=1)\n",
    "\n",
    "    # Linear regression of auxiliary formula\n",
    "    ct_results = smf.ols('ct_resp ~ bev_mu - 1', ct_data).fit()\n",
    "    # Construct confidence interval for alpha, the coefficient of bev_mu\n",
    "    # Overdispersion corresponds to alpha > 0 \n",
    "    alpha_ci95 = ct_results.conf_int(0.05).loc['bev_mu']\n",
    "    print('\\nC-T dispersion test: alpha = {:5.3f}, 95% CI = ({:5.3f}, {:5.3f})'\n",
    "            .format(ct_results.params[0], alpha_ci95.loc[0], alpha_ci95.loc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb82f5-9140-42dc-a0f5-9f4ff88b3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Fit NB.\n",
    "    y_deviant, X_deviant = dmatrices(expr_peace, df_inst_devi_bl_raw.loc[205:444], return_type='dataframe')\n",
    "    nb_results = sm.GLM(y_deviant, X_deviant, family=sm.families.NegativeBinomial(alpha=ct_results.params[0])).fit()\n",
    "    print(nb_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298e7e6f-e12b-4e91-80d9-401608f94794",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Likelihood Ratio test whether Poission or NB is better suited. \n",
    "    from scipy import stats\n",
    "    stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "\n",
    "    def lrtest(llmin, llmax):\n",
    "        lr = 2 * (llmax - llmin)\n",
    "        p = stats.chisqprob(lr, 1) # llmax has 1 dof more than llmin\n",
    "        return lr, p\n",
    "\n",
    "    llf = poi_results.llf\n",
    "    llflitter = nb_results.llf\n",
    "\n",
    "    # Suggest that nb is better fitted\n",
    "    lr, p = lrtest(llf, llflitter)\n",
    "    print('LR test, p value: {:.2f}, {:.4f}'.format(lr, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187e695-f65b-448b-9aa8-a853447da56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Save residuals.\n",
    "    df_inst_devi_bl_raw['inst_resid_prpe'] = nb_results.resid_response\n",
    "    df_inst_devi_bl_raw['inst_resid_prpe'] = df_inst_devi_bl_raw['inst_resid_prpe'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8570b1-392a-4991-9c3a-db2f85540076",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Write to text file.\n",
    "    with open('prpe_nb_summary_091022.txt', 'w') as fh:\n",
    "        fh.write(nb_results.summary().as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57726fe-03d3-4612-85c5-0697419c01af",
   "metadata": {},
   "source": [
    "#### Estimating protests with excessive violence - extracting fitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd5b85-3584-4fe1-90eb-6474ace77d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:    \n",
    "    # Set up regression expressions\n",
    "    expr_exvio = \"\"\"acled_prex_count ~ \n",
    "    vdem_v2x_polyarchy_tlag12 + \n",
    "    vdem_v2x_civlib_tlag12 + \n",
    "    ln_acled_prpe_count_tlag1 + \n",
    "    ln_acled_prex_count_tlag1 +\n",
    "    ln_geb_sb_best_tlag1 +\n",
    "    ln_geb_os_best_tlag1 +\n",
    "    ln_splag_1_1_ged_sb_best_tlag1 +\n",
    "    ln_splag_1_1_ged_os_best_tlag1 +\n",
    "    ln_splag_1_2_acled_prpe_count_tlag1 +\n",
    "    ln_splag_1_2_acled_prex_count_tlag1 +\n",
    "    pgd_pop_gpw_sum \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e484ad6-7a91-4b61-b248-91dc21251937",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Fit poission to get alpha.\n",
    "    y_deviant, X_deviant = dmatrices(expr_exvio, df_inst_devi_bl_raw.loc[205:444], return_type='dataframe')\n",
    "    poi_results2 = sm.GLM(y_deviant, X_deviant, family=sm.families.Poisson()).fit()\n",
    "    print(poi_results2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce4e42-d32f-4882-bffa-095d75e56ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Check for overdispersion\n",
    "    import statsmodels.formula.api as smf\n",
    "    def ct_response(row):\n",
    "        \"Calculate response observation for Cameron-Trivedi dispersion test\"\n",
    "        y = row['acled_prex_count']\n",
    "        m = row['bev_mu']\n",
    "        return ((y - m)**2 - y) / m\n",
    "\n",
    "    ct_data = df_inst_devi_bl_raw.loc[205:444].copy()\n",
    "    ct_data['bev_mu'] = poi_results2.mu\n",
    "    ct_data['ct_resp'] = ct_data.apply(ct_response, axis=1)\n",
    "\n",
    "    # Linear regression of auxiliary formula\n",
    "    ct_results = smf.ols('ct_resp ~ bev_mu - 1', ct_data).fit()\n",
    "    # Construct confidence interval for alpha, the coefficient of bev_mu\n",
    "    # Overdispersion corresponds to alpha > 0 \n",
    "    alpha_ci95 = ct_results.conf_int(0.05).loc['bev_mu']\n",
    "    print('\\nC-T dispersion test: alpha = {:5.3f}, 95% CI = ({:5.3f}, {:5.3f})'\n",
    "            .format(ct_results.params[0], alpha_ci95.loc[0], alpha_ci95.loc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731c423-f971-4d04-bf28-bf7374ccced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Fit negative binomial.\n",
    "    y_deviant, X_deviant = dmatrices(expr_exvio, df_inst_devi_bl_raw.loc[205:444], return_type='dataframe')\n",
    "    nb2_results = sm.GLM(y_deviant, X_deviant,family=sm.families.NegativeBinomial(alpha=ct_results.params[0])).fit()\n",
    "    print(nb2_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459d688-8cf8-4bd6-8c75-184db93bd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Save fitted values.\n",
    "    df_inst_devi_bl_raw['inst_yhat_prex'] = nb2_results.fittedvalues\n",
    "    df_inst_devi_bl_raw['inst_yhat_prex'] = df_inst_devi_bl_raw['inst_yhat_prex'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7305c719-bbb0-4c9e-a309-3db073090c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Write to text file.\n",
    "    with open('prex_nb_summary_101022.txt', 'w') as fh:\n",
    "        fh.write(nb2_results.summary().as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05baee88-d8ed-4fcf-8bc9-fa4f8fa2a3e9",
   "metadata": {},
   "source": [
    "#### Specification of deviation model: Baseline + residuals and fitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3dd0c0-7bc2-40f0-a3bf-c359cec09863",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Add feature to every table including the deviation model.\n",
    "    for df in apply_to_dfs:\n",
    "        print('Name')\n",
    "        if 'devi' in df['Name']:\n",
    "            print('n cols before:', len(df['df'].columns))\n",
    "            df['df'] = pd.concat([df['df'],df_inst_devi_bl_raw['inst_yhat_prex'],df_inst_devi_bl_raw['inst_resid_prpe']],axis=1)\n",
    "            print('n cols after:', len(df['df'].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d176201-6a32-42d7-8f36-092263ff3350",
   "metadata": {},
   "source": [
    "## Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f019b-bd87-471f-afd0-1c1b4d9935d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for dependent variable.\n",
    "if fetch_from_db:\n",
    "    save_table = False\n",
    "    add_protest = False\n",
    "\n",
    "    descr_start = [205,445]\n",
    "    descr_end = 480\n",
    "    cols = ['ged_sb_dummy_dep']\n",
    "\n",
    "    # Write to tex.\n",
    "    if save_table:\n",
    "        for time in descr_start:\n",
    "            tex = datasets[0]['df'].loc[time:descr_end][cols].describe().round(3).to_latex(index=True)\n",
    "            now = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "            meta = f\"\"\"\n",
    "            %Output created by protest_paper.ipynb.\n",
    "            %Descriptive Statistics.\n",
    "            %Produced on {now}.\n",
    "            \\\\\n",
    "            \"\"\"\n",
    "            tex = meta + tex\n",
    "            path_out = os.path.join(output_paths['descriptives'], f\"descr_ged_dummy_sb_{time}_{descr_end}.txt\")\n",
    "            with open(path_out, \"w\") as f:\n",
    "                f.write(tex)\n",
    "            print(f\"Written to {path_out}.\")\n",
    "\n",
    "    for time in descr_start:\n",
    "        print(datasets[0]['df'].loc[time:descr_end][cols].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817ffb0a-aa57-4408-b9f1-65e7d86f55ea",
   "metadata": {},
   "source": [
    "## Filter only relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b78458-4eb0-481d-aacb-3b202647d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read feature list.\n",
    "with open('featlist_protest_paper.yaml', 'r') as file:\n",
    "    full_featlist = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2ac33-1021-417c-b0ee-5fc7fde2edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    for df in datasets:\n",
    "        for feats,colname in zip(full_featlist.keys(),full_featlist.values()):\n",
    "            if df['Name'] == feats:\n",
    "                print(df['Name'])\n",
    "                print('n cols before:', len(df['df'].columns))\n",
    "                df['df'] = df['df'][colname]\n",
    "                print('n cols after:', len(df['df'].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06473ed5-738c-416e-8ae2-245d425a7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Drop duplicated columns\n",
    "    for df in datasets:\n",
    "        df['df'] = organize.getDuplicateColumns(df['df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd40cc9-2a99-49f5-9d26-d26308c70a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    for df in datasets:\n",
    "        for feats,colname in zip(full_featlist.keys(),full_featlist.values()):\n",
    "            if df['Name'] == feats:\n",
    "                print(df['Name'])\n",
    "                print('Matching length:',len(colname)==len(df['df'].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7a4c9-0dc0-4937-b800-61768386c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    for df in datasets:\n",
    "        if df['df'].columns.duplicated().any() == True:\n",
    "                print('Duplicates detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9b65f-7233-4308-8280-ee766d5530f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fetch_from_db:\n",
    "    # Save all the features\n",
    "    save_dict_data = True\n",
    "    if save_dict_data:\n",
    "        with open(os.path.join(output_paths['data'], f\"data_dict_{run_outcome}.p\"), 'wb') as fp:\n",
    "            pickle.dump(datasets, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c4b7f-d11c-40d6-bdd5-7d4ed058432a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86abcad-f75f-4af0-88b2-e24bdae95a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "if fetch_from_db == False:\n",
    "    with open(os.path.join(output_paths['data'], f\"data_dict_incidence.p\"), 'rb') as fp:\n",
    "        datasets = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2dcd69-768a-4d69-b749-46d225b00684",
   "metadata": {},
   "source": [
    "## Define Regressors, Downsampling and Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c9998-e14a-46b1-89a0-fd84109343b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from views_stepshift import Period,Downsampling\n",
    "from views_stepshift import Model,Ensemble\n",
    "from views_stepshift.datautils import assign_into_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ece947-9c56-40a7-b6d6-a0da6029c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "nj=14\n",
    "n_estimators=500\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_jobs=nj, n_estimators=n_estimators, random_state=1308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3e59e-f017-4d4a-9bd1-8709702533bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampling = Downsampling(share_positive = 1.0, share_negative = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c11025-9811-4c33-bf3b-f7db12c59fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [\n",
    "    Period(name=\"A\",train_start=205,train_end=408,predict_start=409,predict_end=444),\n",
    "    Period(name=\"B\",train_start=205,train_end=444,predict_start=445,predict_end=480),\n",
    "]\n",
    "\n",
    "adj_periods = [\n",
    "    Period(name=\"A\",train_start=205,train_end=408-36,predict_start=409-36,predict_end=444-36),\n",
    "    Period(name=\"B\",train_start=205,train_end=444-36,predict_start=445-36,predict_end=480-36),\n",
    "]\n",
    "\n",
    "periods_model = periods # set to adj_periods for robustness test\n",
    "steps = [3,6,12,36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7626c-bb0c-47c9-b259-497bcd0b8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147b2ed-b4ed-4cd6-9d6d-ff82c99efbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in datasets:\n",
    "    print(df['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739bf8a-b1f9-431f-a6e7-f1de11e08251",
   "metadata": {},
   "source": [
    "## Specify Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80987ad7-c778-47ad-b71e-10b12bae087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check parameters.\n",
    "print(depvar, run_outcome, periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ca455-2a5b-48c7-a19d-7925b6fb5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models. \n",
    "models_to_train = [\n",
    "    'baseline_simple',\n",
    "    'econ_nat_bl',\n",
    "    'econ_full_bl',\n",
    "    'inst_elecdemo_bl',\n",
    "    'inst_civlib_bl',\n",
    "    'inst_elect_bl',\n",
    "    'inst_devi_bl',\n",
    "    'pr_naive_bl',\n",
    "    'pr_dynamic_loc_bl',\n",
    "    'pr_dynamic_nat_bl',\n",
    "    'pr_elecdemo_bl',\n",
    "    'pr_civlib_bl',\n",
    "    'pr_elect_bl',\n",
    "    'pr_devi_bl',\n",
    "    'pr_econ_nat_bl',\n",
    "    'pr_econ_full_bl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b49c1f-bebe-42a9-9741-ab1fc329b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelList = []\n",
    "\n",
    "for df in datasets:\n",
    "    for mname in models_to_train:\n",
    "        if df['Name'] == mname:\n",
    "            print(mname)\n",
    "            \n",
    "            for feats,colname in zip(full_featlist.keys(),full_featlist.values()):\n",
    "                if df['Name'] == feats:\n",
    "                    featlist_dep = list(df['df'][colname].columns)\n",
    "                    featlist_f = [x for x in featlist_dep if x != f'{depvar}']\n",
    "            \n",
    "            if 'incidence' in run_outcome:\n",
    "                print('incidence')\n",
    "                ModelList.append(\n",
    "                    Model(\n",
    "                        name = f'protest_{mname}_{run_outcome}',\n",
    "                        col_outcome = depvar,\n",
    "                        cols_features = featlist_f,\n",
    "                        periods = periods_model,\n",
    "                        steps = steps,\n",
    "                        outcome_type = \"prob\",\n",
    "                        downsampling =  downsampling,\n",
    "                        estimator = rf_classifier,\n",
    "                        dir_storage = output_paths['models'],\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            if 'onset' in run_outcome:\n",
    "                print('onset')\n",
    "                ModelList.append(\n",
    "                    Model(\n",
    "                        name = f'protest_{mname}_{run_outcome}',\n",
    "                        col_outcome = depvar,\n",
    "                        cols_features = featlist_f,\n",
    "                        periods = periods_model,\n",
    "                        steps = steps,\n",
    "                        outcome_type = \"prob\",\n",
    "                        downsampling =  downsampling,\n",
    "                        onset_outcome= True,\n",
    "                        onset_window= 6, \n",
    "                        estimator = rf_classifier,\n",
    "                        dir_storage = output_paths['models'],\n",
    "                    )\n",
    "                )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b0308-8c60-4a31-b7c2-070270cfdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ModelList:\n",
    "    print(model.name)\n",
    "    print(model.col_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60444c10-1ff6-4010-a27e-728c58778dbe",
   "metadata": {},
   "source": [
    "## Train, Calibrate, Predict, Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd16b92-423a-420d-a083-f580f384f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving or Loading - dependent on parameter at the top of the notebook.\n",
    "if train:\n",
    "    save_preds = True\n",
    "else:\n",
    "    save_preds = False\n",
    "    \n",
    "print(save_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6aed39-123a-43ac-bb9d-7da6bc149da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.\n",
    "if train:\n",
    "    random.seed(1308)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    for df in datasets:\n",
    "        for model in ModelList:\n",
    "            mname = df['Name']\n",
    "            model.name\n",
    "            if  model.name == f'protest_{mname}_{run_outcome}':\n",
    "                print(f'Fitting {model.name}')\n",
    "\n",
    "                print(datetime.now())\n",
    "                model.fit_estimators(df['df'])\n",
    "                print(datetime.now())\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb887b-3c9a-41ed-b497-18b8d673fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict.\n",
    "if train:\n",
    "    random.seed(1308)\n",
    "    preds_dict = {}\n",
    "    start_time = datetime.now()\n",
    "    for df in datasets:\n",
    "        for model in ModelList:\n",
    "            mname = df['Name']\n",
    "            if  model.name == f'protest_{mname}_{run_outcome}':\n",
    "                print(f'Predicting for {mname}')\n",
    "                print(datetime.now())\n",
    "                preds_dict[model.name] = assign_into_df(df_from=model.predict(df['df']), df_to=df['df']).loc[periods_model[1].predict_start:periods_model[1].predict_end]\n",
    "                print(datetime.now())\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266bc21-1244-486e-bd25-d77c926bf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate.\n",
    "if train:\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for df in datasets:\n",
    "        for model in ModelList:\n",
    "            mname = df['Name']\n",
    "            if  model.name == f'protest_{mname}_{run_outcome}':\n",
    "    \n",
    "                print(f'Evaluating {model.name}')\n",
    "\n",
    "                print(datetime.now())\n",
    "                model.evaluate(df['df'])\n",
    "                print(datetime.now())\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41021cdd-1ff8-4a74-a8e7-9d6a8cfd6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading previously trained models based on the above defined outcome. \n",
    "if save_preds:\n",
    "    for df in datasets:\n",
    "        for model in ModelList:\n",
    "            mname = df['Name']\n",
    "            if  model.name == f'protest_{mname}_{run_outcome}':\n",
    "                \n",
    "                # Save models\n",
    "                model.save(os.path.join(output_paths['models'],f'{mname}_{run_outcome}.joblib'))\n",
    "                \n",
    "    # Save dictonary.\n",
    "    with open(os.path.join(output_paths['predictions'], f\"preds_dict_{run_outcome}.p\"), 'wb') as fp:\n",
    "        pickle.dump(preds_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # Load models.\n",
    "    ModelList = []\n",
    "    models_to_load = models_to_train\n",
    "    \n",
    "    for m in models_to_load:\n",
    "        ModelList.append(Model.load(os.path.join(output_paths['models'],f'{m}_{run_outcome}.joblib')))\n",
    "    \n",
    "    # Load dictonary.\n",
    "    with open(os.path.join(output_paths['predictions'], f\"preds_dict_{run_outcome}.p\"), 'rb') as fp:\n",
    "        preds_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eea998-1243-43ce-b86f-7ebef4d7c0ec",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df332532-1d32-44f8-b703-d646ff21fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(evaltools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7faef6f-733e-42ea-9e0e-8899c3c7b72a",
   "metadata": {},
   "source": [
    "### Evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5bb26-96de-4cc9-b288-16470953ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ModelList:\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411fab4-fbea-49b0-bec9-1e867fd5f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print scores for quick overview.\n",
    "for model in ModelList:\n",
    "    print(model.name)\n",
    "    print(model.scores['B'][3]['uncalibrated']['average_precision'])\n",
    "    print(model.scores['B'][6]['uncalibrated']['average_precision'])\n",
    "    print(model.scores['B'][12]['uncalibrated']['average_precision'])\n",
    "    print(model.scores['B'][36]['uncalibrated']['average_precision'])\n",
    "    print(model.scores['B'][3]['uncalibrated']['area_under_roc'])\n",
    "    print(model.scores['B'][6]['uncalibrated']['area_under_roc'])\n",
    "    print(model.scores['B'][12]['uncalibrated']['area_under_roc'])\n",
    "    print(model.scores['B'][36]['uncalibrated']['area_under_roc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d007f8d-b657-4bd5-afe7-cf65a3e66762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the scores to one df and one table. \n",
    "dfs_scores = []\n",
    "for evalm in ['AP','AUROC','Brier']:\n",
    "    df_scores = evaltools.df_eval_scores(\n",
    "        preds_dict = preds_dict,\n",
    "        model_list = models_to_train, \n",
    "        run_outcome = run_outcome,\n",
    "        ev_name = evalm,\n",
    "        depvar = depvar,\n",
    "        steps = steps,\n",
    "        round_to = 3, \n",
    "        path= os.path.join(output_paths['scores_tables'], f\"eval_{evalm}_{run_outcome}.tex\"\n",
    "                          )\n",
    "    )\n",
    "    # Store in list.\n",
    "    dfs_scores.append(df_scores)\n",
    "    \n",
    "# Rename\n",
    "rename_models_dict = {\n",
    "    f'baseline_simple':'M0',\n",
    "    f'econ_nat_bl': 'M8 w/o pr',\n",
    "    f'econ_full_bl': 'M9 w/o pr',\n",
    "    f'inst_elecdemo_bl': 'M4 w/o pr',\n",
    "    f'inst_civlib_bl' : 'M5 w/o pr',\n",
    "    f'inst_elect_bl' : 'M6 w/o pr',\n",
    "    f'inst_devi_bl': 'M7 w/o pr',\n",
    "    f'inst_election_econ_national_bl{run_outcome}': 'M6M8 w/o pr',\n",
    "    #f'polinst_election_econ_full_bl': 'M6M9 w/o pr',\n",
    "    #f'polinst_devi_econ_national_bl': 'M7M8 w/o pr',\n",
    "    #f'polinst_devi_econ_full_bl': 'M7M9 w/o pr',\n",
    "    f'pr_naive_bl': 'M1',\n",
    "    f'pr_dynamic_loc_bl': 'M2',\n",
    "    f'pr_dynamic_nat_bl': 'M3',\n",
    "    f'pr_elecdemo_bl': 'M4',\n",
    "    f'pr_civlib_bl': 'M5',\n",
    "    f'pr_elect_bl': 'M6',\n",
    "    f'pr_devi_bl': 'M7',\n",
    "    f'pr_econ_nat_bl': 'M8',\n",
    "    f'pr_econ_full_bl': 'M9',\n",
    "    #f'pr_polinst_election_econ_national_bl_{run_outcome}': 'M6M8',\n",
    "    #f'pr_polinst_election_econ_full_bl{run_outcome}': 'M6M9',\n",
    "    #f'pr_polinst_devi_econ_national_bl{run_outcome}': 'M7M8',\n",
    "    #f'pr_polinst_devi_econ_full_bl{run_outcome}': 'M7M9'\n",
    "}\n",
    "reorder_models = [\n",
    "    'M0',\n",
    "    #'M0',\n",
    "    'M1',\n",
    "    'M2',\n",
    "    'M3',\n",
    "    'M4',\n",
    "    'M4 w/o pr',\n",
    "    'M5',\n",
    "    'M5 w/o pr',\n",
    "    'M6',\n",
    "    'M6 w/o pr',\n",
    "    'M7',\n",
    "    'M7 w/o pr',\n",
    "    'M8',\n",
    "    'M8 w/o pr',\n",
    "    'M9',\n",
    "    'M9 w/o pr',\n",
    "    'M6M8',\n",
    "    'M6M8 w/o pr',\n",
    "    'M6M9',\n",
    "    'M6M9 w/o pr',\n",
    "    'M7M8',\n",
    "    'M7M8 w/o pr',\n",
    "    'M7M9',\n",
    "    'M7M9 w/o pr',\n",
    "]\n",
    "\n",
    "# Make into single df.\n",
    "dfs_scores_all = pd.concat(dfs_scores,axis=1)\n",
    "dfs_scores_all = dfs_scores_all.rename(index=rename_models_dict)\n",
    "dfs_scores_all = dfs_scores_all.reindex(reorder_models).dropna()\n",
    "dfs_scores_all.to_csv(os.path.join(output_paths['scores_tables'], f\"eval_all_{run_outcome}.csv\"))\n",
    "print(dfs_scores_all)\n",
    "\n",
    "# Write to tex. file. \n",
    "path = os.path.join(output_paths['scores_tables'], f\"eval_all_{run_outcome}.tex\")\n",
    "tex = dfs_scores_all.reset_index().to_latex(index=False)\n",
    "\n",
    "# Get meta infromation\n",
    "now = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "meta = f\"\"\"\n",
    "%Date: {now}\n",
    "%Output created by protest_paper.ipynb.\n",
    "%Compare eval metrics for all models.\n",
    "\\\\\n",
    "\"\"\"\n",
    "tex = meta + tex\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(tex)\n",
    "print(f\"Wrote scores table to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e5d8f-9856-43a4-a9c1-1e0689950377",
   "metadata": {},
   "source": [
    "### Parallel coordinate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03352b2c-5c45-4c7c-8bfb-2c999b479cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make coordinate plots\n",
    "dfs_coords = pd.read_csv(os.path.join(output_paths['scores_tables'], f\"eval_all_{run_outcome}.csv\"),index_col=[0],header=[0,1], skipinitialspace=True)\n",
    "dfs_coords_copy = dfs_coords.swaplevel(axis=1)\n",
    "dfs_coords_copy = dfs_coords_copy.reindex([('3', 'AP'),\n",
    " ('3', 'AUROC'),\n",
    " ('3', 'Brier'),\n",
    " ('6', 'AP'),\n",
    " ('6', 'AUROC'),\n",
    " ('6', 'Brier'),\n",
    " ('12', 'AP'),\n",
    " ('12', 'AUROC'),\n",
    " ('12', 'Brier'),\n",
    " ('36', 'AP'),\n",
    " ('36', 'AUROC'),\n",
    " ('36', 'Brier')], axis=1)\n",
    "dfs_coords_copy = dfs_coords_copy[dfs_coords_copy.index.isin(['M0','M1','M2'])]\n",
    "\n",
    "evaltools.plot_parcoord_allsteps(\n",
    "    df = dfs_coords_copy,\n",
    "    steps = ['3','6','12','36'],\n",
    "    reverse = True,\n",
    "    cmap='Dark2',\n",
    "    legend_label=['M0','M1','M2',],\n",
    "    path = os.path.join(output_paths[\"coord_plots\"], f\"coord_simple_{run_outcome}.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae95b5-a8a8-41c4-b4e8-bb3be8da32d2",
   "metadata": {},
   "source": [
    "### Boostrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b7db6-0a1d-482f-a95e-5f1b0e61c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_bs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7414ba-cf2c-43e2-b221-e14f23a28375",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_bs:\n",
    "    # Make list of all model names\n",
    "    all_model_names = []\n",
    "    for model in ModelList:\n",
    "        all_model_names.append(model.name)\n",
    "\n",
    "    # Select models.\n",
    "    coord_models_names = []\n",
    "    matchers = [\n",
    "        f'protest_baseline_simple_{run_outcome}',\n",
    "        f'protest_pr_naive_bl_{run_outcome}',\n",
    "        f'protest_pr_dynamic_loc_bl_{run_outcome}',\n",
    "        f'protest_pr_dynamic_nat_bl_{run_outcome}',\n",
    "        f'protest_inst_elecdemo_bl_{run_outcome}',\n",
    "        f'protest_pr_elecdemo_bl_{run_outcome}',\n",
    "        f'protest_inst_civlib_bl_{run_outcome}',\n",
    "        f'protest_pr_civlib_bl_{run_outcome}',\n",
    "        f'protest_inst_elect_bl_{run_outcome}',\n",
    "        f'protest_pr_elect_bl_{run_outcome}',\n",
    "        f'protest_inst_devi_bl_{run_outcome}',\n",
    "        f'protest_pr_devi_bl_{run_outcome}',\n",
    "        f'protest_econ_nat_bl_{run_outcome}',\n",
    "        f'protest_pr_econ_nat_bl_{run_outcome}',\n",
    "        f'protest_econ_full_bl_{run_outcome}',\n",
    "        f'protest_pr_econ_full_bl_{run_outcome}'\n",
    "    ]\n",
    "\n",
    "    coord_models_names = [s for s in all_model_names if any(xs in s for xs in matchers)]\n",
    "\n",
    "    models_to_boot = []\n",
    "    for model in ModelList:\n",
    "        if model.name in coord_models_names:\n",
    "            models_to_boot.append(model.name)\n",
    "\n",
    "\n",
    "    eval_fun = 'average_precision'\n",
    "    steps = steps\n",
    "\n",
    "    dfs_boots=[]\n",
    "    for model_to_boot in models_to_boot:\n",
    "        print(model_to_boot)\n",
    "        for step in steps:\n",
    "            print(step)\n",
    "            df_boots = evaltools.boot_evalmetric(\n",
    "                model_name = model_to_boot,\n",
    "                preds_dict = preds_dict, \n",
    "                depvar = depvar,\n",
    "                step=step,\n",
    "                eval_fun = eval_fun,\n",
    "                set_seed = 1308,\n",
    "                n_bootstraps=1000,\n",
    "            )\n",
    "\n",
    "            dfs_boots.append(df_boots)\n",
    "\n",
    "    df_boots_all = pd.concat(dfs_boots,axis=1) \n",
    "    df_boots_all.to_csv(os.path.join(output_paths[\"bootstrapped\"], f\"boots_ap_{run_outcome}.csv\"))\n",
    "    print('df written to', f\"boots_ap_{run_outcome}.csv\")\n",
    "    \n",
    "else:\n",
    "    df_boots_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4584f-b468-4ad5-a464-6e1601d74932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boots_all = pd.read_csv(os.path.join(output_paths[\"bootstrapped\"], f\"boots_ap_{run_outcome}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb695d-845e-4c3d-bf09-f48dfd6bdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters.\n",
    "\n",
    "titles = [\n",
    "    'M1 vs M0',\n",
    "    'M2 vs M1',\n",
    "    'M3 vs M1',\n",
    "    'M2 vs M0',\n",
    "    'M3 vs M0',\n",
    "    'M4 vs M2',\n",
    "    'M5 vs M2',\n",
    "    'M6 vs M2',\n",
    "    'M7 vs M2',\n",
    "    'M8 vs M2',\n",
    "    'M9 vs M2',\n",
    "    'M4 vs M4 w/o pr ',\n",
    "    'M5 vs M5 w/o pr',\n",
    "    'M6 vs M6 w/o pr',\n",
    "    'M7 vs M7 w/o pr',\n",
    "    'M8 vs M8 w/o pr',\n",
    "    'M9 vs M9 w/o pr',\n",
    "\n",
    "]\n",
    "\n",
    "model1 =[\n",
    "    f'protest_pr_naive_bl_{run_outcome}_average_precision_', #M1\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', #M2\n",
    "    f'protest_pr_dynamic_nat_bl_{run_outcome}_average_precision_', # M3\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', #M2\n",
    "    f'protest_pr_dynamic_nat_bl_{run_outcome}_average_precision_', # M3\n",
    "    f'protest_pr_elecdemo_bl_{run_outcome}_average_precision_', #M4\n",
    "    f'protest_pr_civlib_bl_{run_outcome}_average_precision_', #M5\n",
    "    f'protest_pr_elect_bl_{run_outcome}_average_precision_', #M6\n",
    "    f'protest_pr_devi_bl_{run_outcome}_average_precision_', #M7\n",
    "    f'protest_pr_econ_nat_bl_{run_outcome}_average_precision_', #M8\n",
    "    f'protest_pr_econ_full_bl_{run_outcome}_average_precision_', #M9\n",
    "    f'protest_pr_elecdemo_bl_{run_outcome}_average_precision_', #M4\n",
    "    f'protest_pr_civlib_bl_{run_outcome}_average_precision_', #M5\n",
    "    f'protest_pr_elect_bl_{run_outcome}_average_precision_', #M6\n",
    "    f'protest_pr_devi_bl_{run_outcome}_average_precision_', #M7\n",
    "    f'protest_pr_econ_nat_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_pr_econ_full_bl_{run_outcome}_average_precision_',\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "model2 = [\n",
    "    f'protest_baseline_simple_{run_outcome}_average_precision_', # M0\n",
    "    f'protest_pr_naive_bl_{run_outcome}_average_precision_', #M1\n",
    "    f'protest_pr_naive_bl_{run_outcome}_average_precision_', #M1\n",
    "    f'protest_baseline_simple_{run_outcome}_average_precision_', # M0\n",
    "    f'protest_baseline_simple_{run_outcome}_average_precision_', # M0\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_inst_elecdemo_bl_{run_outcome}_average_precision_', #M4 wo pr\n",
    "    f'protest_inst_civlib_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_inst_elect_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_inst_devi_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_econ_nat_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_econ_full_bl_{run_outcome}_average_precision_',  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9e012-9ccb-474b-85ca-fa0b16ecdbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters.\n",
    "legendtrue = False\n",
    "\n",
    "# Adjust these parameters for different outcomes \n",
    "if run_outcome == 'incidence':\n",
    "    ymin = -0.02 \n",
    "    ymax = 0.12\n",
    "\n",
    "# Onset\n",
    "if run_outcome == 'onset':\n",
    "    ymin = -0.1 # Adjust these parameters for different outcomes \n",
    "    ymax = 0.14 # Adjust these parameters for different outcomes \n",
    "\n",
    "evaltools.plot_bootstrapped_diff(\n",
    "    df=df_boots_all,\n",
    "    titles=titles,\n",
    "    modellist1=model1,\n",
    "    modelllist2=model2,\n",
    "    legendtrue=legendtrue,\n",
    "    steps=steps,\n",
    "    ymin=ymin,\n",
    "    ymax=ymax,\n",
    "    save_as=run_outcome,\n",
    "    path_out=output_paths[\"bootstrapped\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c4ef0-5927-4a64-a734-7a12d08bbbab",
   "metadata": {},
   "source": [
    "### Prediction maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd492222-3626-4b88-99b1-d9c7923bc585",
   "metadata": {},
   "source": [
    "#### Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427c41e-81f2-4c7f-8c9b-0666ca5f810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from views_mapper2.mapper2 import *\n",
    "from views_mapper2.BBoxWriter import *\n",
    "from views_mapper2.dictionary_writer import *\n",
    "from views_mapper2.label_writer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de081bdb-6efc-4b8b-8b39-2779df508e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch geometries from database\n",
    "gdf_mapping = organize.fetch_gdf()\n",
    "gdf_mapping = gdf_mapping.loc[445:480]\n",
    "gdf_mapping.to_csv(os.path.join(output_paths[\"data\"], f\"gdf_mapping.csv\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044f706-c216-4eaa-8a1c-ca9d3bad9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import sqlalchemy as sa\n",
    "from ingester3.config import source_db_path\n",
    "engine = sa.create_engine(source_db_path)\n",
    "\n",
    "gdf_ci_master = gpd.GeoDataFrame.from_postgis(\n",
    "    \"SELECT id as country_id, in_africa, in_me, geom FROM prod.country\",\n",
    "    engine,\n",
    "    geom_col='geom'\n",
    ")\n",
    "gdf_ci_master = gdf_ci_master.to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8431238-f238-4b97-8366-23257e7ca0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match steps with months.\n",
    "times = [447,450,456,480] \n",
    "allsteps = steps\n",
    "times_steps = dict(zip(times, allsteps)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6d854-b626-4cf5-a8f8-eb73641bce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scale\n",
    "proba_dict= {'0.1%':0.001, #'0.2%':0.002, '0.5%': 0.005,\n",
    "               #'1%':0.01, \n",
    "             '2%':0.02, '5%': 0.05,\n",
    "               '10%':0.1, '20%':0.2, '40%': 0.4,\n",
    "               '60%':0.6, '80%':0.8, '90%': 0.9,\n",
    "               '95%':0.95, '99%':0.99, \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e2909-a4f3-4186-90e2-2629f1b35a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_to_train:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ca27f-a9cf-4a35-b795-5a699dae6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [models_to_train[0]]:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(mname)\n",
    "    mname_plt = model.replace('simple_','')\n",
    "    print(mname_plt)\n",
    "\n",
    "    for key, value in times_steps.items():\n",
    "        \n",
    "        mapdf = pd.concat([preds_dict[mname][f'ss_{mname}_{value}'],preds_dict[mname][depvar],gdf_mapping],axis=1)  \n",
    "        mapdf = gpd.GeoDataFrame(mapdf, geometry=\"geometry\")\n",
    "        print(key,value)\n",
    "        \n",
    "        pgm_masked=Mapper2(\n",
    "            width=20,\n",
    "            height=20,\n",
    "            frame_on=True,\n",
    "            #map_scale='logodds',\n",
    "            #title='mask with layers',\n",
    "            bbox=bbox_from_cid_region('africa'),\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key],\n",
    "            map_dictionary = proba_dict,\n",
    "            cmap = 'rainbow',\n",
    "            transparency = 1,\n",
    "            #map_scale=proba_scale,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            column=f'ss_{mname}_{value}',\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key][mapdf.loc[key][depvar]==1].geometry.centroid,\n",
    "            marker=\"o\",\n",
    "            markersize=5,\n",
    "            #column=f'actuals_step{value}',\n",
    "            color=\"black\"\n",
    "        ).add_views_textbox(\n",
    "            text=f'Run: protest \\nModel: {model}, \\nMonth: {key} (Step {value})',\n",
    "            textsize=20)\n",
    "        \n",
    "        ax = pgm_masked.ax\n",
    "        gdf_ci_master.plot(ax=ax,edgecolor='grey',linewidth=0.2,facecolor='None')\n",
    "\n",
    "        pgm_masked.save(output_paths['maps']+f'/baseline_step{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90153c60-2ce4-48f7-a9eb-3057e14c7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [models_to_train[0]]:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(mname)\n",
    "    mname_plt = model.replace('simple_','')\n",
    "    print(mname_plt)\n",
    "\n",
    "    for key, value in times_steps.items():\n",
    "        \n",
    "        mapdf = pd.concat([preds_dict[mname][f'ss_{mname}_{value}'],preds_dict[mname][depvar],gdf_mapping],axis=1)  \n",
    "        mapdf = gpd.GeoDataFrame(mapdf, geometry=\"geometry\")\n",
    "        print(key,value)\n",
    "        \n",
    "        pgm_masked=Mapper2(\n",
    "            width=20,\n",
    "            height=20,\n",
    "            frame_on=True,\n",
    "            #map_scale='logodds',\n",
    "            #title='mask with layers',\n",
    "            bbox=[22.6716, 48.819, -2.8909, 16.2484],\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key],\n",
    "            map_dictionary = proba_dict,\n",
    "            cmap = 'rainbow',\n",
    "            transparency = 1,\n",
    "            #map_scale=proba_scale,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            column=f'ss_{mname}_{value}',\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key][mapdf.loc[key][depvar]==1].geometry.centroid,\n",
    "            marker=\"o\",\n",
    "            markersize=5,\n",
    "            #column=f'actuals_step{value}',\n",
    "            color=\"black\"\n",
    "        ).add_views_textbox(\n",
    "            text=f'Run: protest \\nModel: {model}, \\nMonth: {key} (Step {value})',\n",
    "            textsize=20)\n",
    "        \n",
    "        ax = pgm_masked.ax\n",
    "        gdf_ci_master.plot(ax=ax,edgecolor='grey',linewidth=0.2,facecolor='None')\n",
    "\n",
    "        pgm_masked.save(output_paths['maps']+f'/baseline_step{value}_zoom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d7ac1-d4e3-4e2e-ad6b-6cf8d5641d22",
   "metadata": {},
   "source": [
    "#### Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a48dc-336d-4803-b99e-c60546e3d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = True\n",
    "baselinename = f'protest_baseline_simple_{run_outcome}'\n",
    "\n",
    "for model in models_to_train:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(model)\n",
    "    \n",
    "    for s in steps:\n",
    "        preds_dict[mname][f'diff_{mname}_{s}'] =  preds_dict[mname][f'ss_{mname}_{s}']-preds_dict[baselinename][f'ss_{baselinename}_{s}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2a27d-5080-4bda-b544-ef8359fcacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scale\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "proba_dict= {'-90%':-0.9, \n",
    "             '-60%':-0.6,\n",
    "             '-40%':-0.4,\n",
    "             '-20%':-0.2,\n",
    "             '0%':0,\n",
    "             '20%':0.2,\n",
    "             '40%':0.4,\n",
    "             '60%':0.6,\n",
    "             '90%':0.9,\n",
    "              }\n",
    "cmap = plt.get_cmap('seismic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e0dd4-95c5-4ec6-ae27-a5b034707556",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_to_train:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(mname)\n",
    "    mname_plt = model.replace('simple_','')\n",
    "    print(mname_plt)\n",
    "\n",
    "    for key, value in times_steps.items():\n",
    "        \n",
    "        mapdf = pd.concat([preds_dict[mname][f'diff_{mname}_{s}'],preds_dict[mname][depvar],gdf_mapping],axis=1)  \n",
    "        mapdf = gpd.GeoDataFrame(mapdf, geometry=\"geometry\")\n",
    "        print(key,value)\n",
    "        \n",
    "        pgm_masked=Mapper2(\n",
    "            width=20,\n",
    "            height=20,\n",
    "            frame_on=True,\n",
    "            #map_scale='logodds',\n",
    "            #title='mask with layers',\n",
    "            bbox=bbox_from_cid_region('africa'),\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key],\n",
    "            map_dictionary = proba_dict,\n",
    "            cmap = cmap,\n",
    "            transparency = 1,\n",
    "            #map_scale=proba_scale,\n",
    "            #edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            column=f'diff_{mname}_{s}',\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key][mapdf.loc[key][depvar]==1].geometry.centroid,\n",
    "            marker=\"o\",\n",
    "            markersize=5,\n",
    "            #column=f'actuals_step{value}',\n",
    "            color=\"black\"\n",
    "        ).add_views_textbox(\n",
    "            text=f'Run: protest \\nModel: {model}, \\nMonth: {key} (Step {value})',\n",
    "            textsize=20)\n",
    "        \n",
    "        ax = pgm_masked.ax\n",
    "        gdf_ci_master.plot(ax=ax,edgecolor='grey',linewidth=0.2,facecolor='None')\n",
    "\n",
    "        pgm_masked.save(output_paths['maps']+f'/diff_{model}_step{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5e6cb-e27a-4bfd-befa-3967fb8a922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_to_train:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(mname)\n",
    "    mname_plt = model.replace('simple_','')\n",
    "    print(mname_plt)\n",
    "\n",
    "    for key, value in times_steps.items():\n",
    "        \n",
    "        mapdf = pd.concat([preds_dict[mname][f'diff_{mname}_{s}'],preds_dict[mname][depvar],gdf_mapping],axis=1)  \n",
    "        mapdf = gpd.GeoDataFrame(mapdf, geometry=\"geometry\")\n",
    "        print(key,value)\n",
    "        \n",
    "        pgm_masked=Mapper2(\n",
    "            width=20,\n",
    "            height=20,\n",
    "            frame_on=True,\n",
    "            #map_scale='logodds',\n",
    "            #title='mask with layers',\n",
    "            bbox=[22.6716, 48.819, -2.8909, 16.2484],\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key],\n",
    "            map_dictionary = proba_dict,\n",
    "            cmap = cmap,\n",
    "            transparency = 1,\n",
    "            #map_scale=proba_scale,\n",
    "            #edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            column=f'diff_{mname}_{s}',\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key][mapdf.loc[key][depvar]==1].geometry.centroid,\n",
    "            marker=\"o\",\n",
    "            markersize=5,\n",
    "            #column=f'actuals_step{value}',\n",
    "            color=\"black\"\n",
    "        ).add_views_textbox(\n",
    "            text=f'Run: protest \\nModel: {model}, \\nMonth: {key} (Step {value})',\n",
    "            textsize=20)\n",
    "        \n",
    "        ax = pgm_masked.ax\n",
    "        gdf_ci_master.plot(ax=ax,edgecolor='grey',linewidth=0.2,facecolor='None')\n",
    "\n",
    "        pgm_masked.save(output_paths['maps']+f'/diff_{model}_step{value}_zoom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a39a3-fdef-4202-a04f-f389587840fd",
   "metadata": {},
   "source": [
    "### ICE/PDP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337e498-2f7d-4ae7-b313-b81c2631df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in datasets:\n",
    "    print(df['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f030a25-16a5-4a7d-bdac-b00d038fd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from time import time\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "m = ModelList[14] # Full pr_econ\n",
    "pd_df = datasets[18]['df']\n",
    "partition='B'\n",
    "step = 3\n",
    "featlist = [\n",
    "    'decay_ts_6_acled_prex_dummy',\n",
    "    'decay_ts_6_acled_prin_dummy',\n",
    "    'decay_ts_6_acled_prpe_dummy',\n",
    "    'decay_ts_6_acled_prri_dummy'\n",
    "]\n",
    "\n",
    "# Plot.\n",
    "modelname = 'M9'\n",
    "sample_n = 1000\n",
    "\n",
    "# Change paramters here for new estimation.\n",
    "save_fig = True\n",
    "create_save_pickle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc8475-5f76-4e50-bd48-eb39907b3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pickle\n",
    "if create_save_pickle:\n",
    "    for feat in featlist:\n",
    "        print(feat)\n",
    "        pd_output = partial_dependence(\n",
    "            estimator = m.estimators.get(period_name=partition, step=step), \n",
    "            X = pd_df.loc[periods[1].train_start:periods[1].train_end][m.cols_features],\n",
    "            features=feat, \n",
    "            response_method='auto', \n",
    "            percentiles=(0, 1), \n",
    "            grid_resolution=20,  \n",
    "            kind='both',\n",
    "        )\n",
    "\n",
    "\n",
    "        pd_outputdict = dict(pd_output)\n",
    "        a_file = open(output_paths['features'] + f\"/pdp_{modelname}_s3_{feat}.pkl\", \"wb\")\n",
    "        pickle.dump(pd_outputdict, a_file)\n",
    "        a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96552f9-9847-40d3-bd89-749890716bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read pickle\n",
    "featlist = [\n",
    "    'decay_ts_6_acled_prex_dummy',\n",
    "    'decay_ts_6_acled_prin_dummy',\n",
    "    'decay_ts_6_acled_prpe_dummy',\n",
    "    'decay_ts_6_acled_prri_dummy'\n",
    "]\n",
    "\n",
    "modelname = 'M9'\n",
    "pd_df = datasets[18]['df']\n",
    "sample_n = 1000\n",
    "step = 3\n",
    "\n",
    "save_fig = True\n",
    "\n",
    "for feat in featlist:\n",
    "    pd_output = pd.read_pickle(output_paths['features'] + f\"/pdp_{modelname}_s3_{feat}.pkl\")\n",
    "    \n",
    "    print('Making Plot.')\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    pdp_sample = pd.DataFrame(pd_output['individual'][0]).sample(n=sample_n,replace=True)\n",
    "    pdp_sample = pdp_sample.apply(lambda row: row-pdp_sample.iloc[:, 0])\n",
    "    plt.plot(pdp_sample.T,color='lightgrey',linewidth=0.5)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "    xvals_cent = []\n",
    "    for i in pd_output['average'][0]:\n",
    "        xvals_cent.append(i-pd_output['average'][0][0])\n",
    "\n",
    "    # Add average.\n",
    "    plt.plot(pd_output['values'][0],xvals_cent,color='black')\n",
    "\n",
    "    # Add horizontal line. \n",
    "    ax.axhline(y=0, color='dimgrey', linestyle='--', lw=2)\n",
    "\n",
    "    # Add rug plot.\n",
    "    y_min, y_max = (-0.15, 0.15)\n",
    "    ax.plot(pd_df.loc[periods[1].train_start:periods[1].train_end][feat], [y_min]*len(pd_df.loc[periods[1].train_start:periods[1].train_end][feat]), '|', color='black',lw=0.5)\n",
    "\n",
    "    ax.set_ylim(-0.15, 0.15)\n",
    "\n",
    "    # Add title.\n",
    "    plt.title(f'Centered ICE plot for {feat}\\n {modelname}, step={step},\\n sample size={sample_n}, grid points={len(pdp_sample.columns)}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        fig.savefig(output_paths['features']+ f\"{modelname}_s3_{feat}_adj.png\",\n",
    "                    dpi=200,\n",
    "                    facecolor=\"white\",\n",
    "                    bbox_inches=\"tight\",\n",
    "                )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3c2b3-c7bc-4991-9a72-5ab55cb175af",
   "metadata": {},
   "source": [
    "### PR-Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fbfef-b529-4936-8685-8ff1ac8fe75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.cm.get_cmap('Dark2')\n",
    "colors = cm.colors\n",
    "step = 3\n",
    "fig_scale = 1\n",
    "\n",
    "\n",
    "# Dictonary with model comparisons to be plotted.\n",
    "dictoplots = [\n",
    "    {\n",
    "        'baseline_simple':'M0',\n",
    "        'pr_naive_bl':'M1'\n",
    "    },\n",
    "    {\n",
    "        'pr_naive_bl':'M1',\n",
    "        'pr_dynamic_loc_bl':'M2'\n",
    "    },\n",
    "    {\n",
    "        'pr_naive_bl':'M1',\n",
    "        'pr_dynamic_nat_bl':'M3'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_elecdemo_bl':'M4',\n",
    "        'inst_elecdemo_bl':'M4 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_civlib_bl':'M5',\n",
    "        'inst_civlib_bl':'M5 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_elect_bl':'M6',\n",
    "        'inst_elect_bl':'M6 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_devi_bl':'M7',\n",
    "        'inst_devi_bl':'M7 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_econ_nat_bl':'M8',\n",
    "        'econ_nat_bl':'M8 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_econ_full_bl':'M9',\n",
    "        'econ_full_bl':'M9 w/o pr'\n",
    "    },\n",
    "]\n",
    "    \n",
    "for dic in dictoplots:\n",
    "\n",
    "    namelist = []\n",
    "    for i in dic.values():\n",
    "        namelist.append(i)\n",
    "    namelist = ''.join(namelist).replace('/','').replace(' ', '')\n",
    "\n",
    "    # Figure\n",
    "    fig = plt.figure(figsize=(8 * fig_scale, 8 * fig_scale))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    for model,clr, mname in zip(\n",
    "        dic.keys(),colors, dic.values()):\n",
    "        # Compute fpr, tpr, thresholds\n",
    "        precision, recall, _ = precision_recall_curve(preds_dict[f'protest_{model}_{run_outcome}'][depvar], preds_dict[f'protest_{model}_{run_outcome}'][f'ss_protest_{model}_{run_outcome}_{step}'])\n",
    "\n",
    "        #Plot\n",
    "        plt.plot(recall,precision,label=f'{mname}',color=clr)\n",
    "        plt.legend(title=\"Models\")\n",
    "        #plt.xlim([0.0, 1.0])\n",
    "        #plt.ylim([0.0, 1.02])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        \n",
    "        plt.savefig(\n",
    "            output_paths['pr_curves'] + f\"/pr_curve_{namelist}_s{step}.png\", bbox_inches='tight',dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eaeb38-9160-438a-8270-665d6e58402e",
   "metadata": {},
   "source": [
    "### Bi-separation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e19eb8-bf90-4fe8-b8e9-d767f2682ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_s = [445,454,463,472]\n",
    "time_e = [453,462,471,480]\n",
    "steps_sp= 3\n",
    "models1 = ['baseline_simple','pr_naive_bl','pr_naive_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl']\n",
    "models2 = ['pr_naive_bl','pr_dynamic_loc_bl','pr_dynamic_nat_bl','pr_elecdemo_bl','pr_civlib_bl','pr_elect_bl','pr_devi_bl','pr_econ_nat_bl','pr_econ_full_bl']\n",
    "models1_names = ['M0','M1','M1','M2','M2','M2','M2','M2','M2']\n",
    "models2_names = ['M1','M2','M3','M4','M5','M6','M7','M8','M9']\n",
    "\n",
    "df_filtered = preds_dict['protest_baseline_simple_incidence'].loc[450:452].reset_index().rename(columns={'priogrid_gid':'pg_id'})\n",
    "filtered_bbox = df_filtered[(df_filtered.pgm.lat>-4.3) & (df_filtered.pgm.lat<16.4) & (df_filtered.pgm.lon>22) & (df_filtered.pgm.lon<50)]\n",
    "\n",
    "#for t_s, t_e in zip(time_s,time_e):\n",
    "for m1,m2,m1names,m2names in zip(models1,models2,models1_names,models2_names):\n",
    "    for step in [steps_sp]:\n",
    "\n",
    "        dfm1,dfm2 = preds_dict[f'protest_{m1}_{run_outcome}'].reset_index(),preds_dict[f'protest_{m2}_{run_outcome}'].reset_index()\n",
    "        df_sp_1 = dfm1[dfm1['priogrid_gid'].isin(filtered_bbox.pg_id.unique())].set_index(['month_id','priogrid_gid']).drop(depvar,axis=1)\n",
    "        df_sp_2 = dfm2[dfm2['priogrid_gid'].isin(filtered_bbox.pg_id.unique())].set_index(['month_id','priogrid_gid'])\n",
    "\n",
    "        df_sp = pd.concat([df_sp_1,df_sp_2],axis=1)\n",
    "        print('Plot')\n",
    "        spl.BiseparationPlot(\n",
    "            df = df_sp.loc[447],\n",
    "            x = f'ss_protest_{m1}_{run_outcome}_{step}', \n",
    "            y = f'ss_protest_{m2}_{run_outcome}_{step}', \n",
    "            obs = f'{depvar}',\n",
    "            lab = 'priogrid_gid',\n",
    "            markersize=50,\n",
    "            title = f\"{m1names} versus {m2names}, step {step}\",\n",
    "            path = output_paths['bisep'] + f\"/sp_{m1names}_{m2names}_s{step}_447.png\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
