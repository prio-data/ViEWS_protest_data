{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b27daa-ddee-4c92-96fc-ed6051b2e3c4",
   "metadata": {},
   "source": [
    "# 02 Predicting Armed Conflict Using Protest Data - Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06fec9c-0eaf-4358-882f-207727476beb",
   "metadata": {},
   "source": [
    "<b>Notebook description:</b> This notebook belongs to the replication files for the \"Predicting Armed Conflict Using Protest Data\" article. The notebook loads the data extracted from the VIEWS database (see \"predicting_armed_conflict_using_protest_data_01_queries_data.ipynb\").\n",
    "\n",
    "<b>Note:</b> The notebook requires the user to check and install the required packages listed under [Loading modules](#modules). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d231cee4-a9b7-48e7-8efa-d039bbd756f6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* [Importing modules](#modules)\n",
    "* [Defining folder structure](#define_folders)\n",
    "* [Specifying global parameters](#specify_parameters)\n",
    "* [Training models](#trainpredict)\n",
    "* [Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a1c06-534b-4cd2-b6e1-007fa76c96c8",
   "metadata": {},
   "source": [
    "## Loading modules<a class=\"anchor\" id=\"modules\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b088405-4587-43c0-abd7-f099aa5dc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Views-related packages \n",
    "import views_dataviz\n",
    "from views_dataviz.map import mapper, utils\n",
    "from views_stepshift import Period,Downsampling #(https://pypi.org/project/views-stepshift/)\n",
    "from views_stepshift import Model,Ensemble\n",
    "from views_stepshift.datautils import assign_into_df\n",
    "\n",
    "# Mapper (https://pypi.org/project/views-mapper2/)\n",
    "from views_mapper2.mapper2 import *\n",
    "from views_mapper2.BBoxWriter import *\n",
    "from views_mapper2.dictionary_writer import *\n",
    "from views_mapper2.label_writer import *\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.inspection import partial_dependence\n",
    "import sepplotlib as spl\n",
    "\n",
    "# Additional transforms\n",
    "import predicting_armed_conflict_using_protest_data_transforms as transforms\n",
    "import predicting_armed_conflict_using_protest_data_models as organize \n",
    "import predicting_armed_conflict_using_protest_data_utils as utils \n",
    "import predicting_armed_conflict_using_protest_data_eval as evaltools\n",
    "\n",
    "# Other packages\n",
    "import importlib\n",
    "import os\n",
    "import yaml\n",
    "import pickle \n",
    "from patsy import dmatrices\n",
    "import random\n",
    "from time import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c057f70-5cc0-4670-ba01-950d95955dd5",
   "metadata": {},
   "source": [
    "## Defining folder structure<a class=\"anchor\" id=\"define_folders\"></a>\n",
    "\n",
    "To follow the replication step by step, we recommend you to download and store the folder structure available here https://www.dropbox.com/sh/jiyjo6ic12mv6j6/AABogy4M1nNBtJTgvDysQcO7a?dl=0.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01e29b-3b85-49f9-a028-66c985fc7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your username \n",
    "username= 'maxle647' #'yourusername'\n",
    "\n",
    "# Define path.\n",
    "folder_path = f'/Users/{username}/Dropbox (ViEWS)/Protest article replication' #protest_views3'  # Change path\n",
    "print('Folder path:', folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56739ca-2392-494f-8817-4281878f5b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if they do not yet exist. \n",
    "\n",
    "if not os.path.isdir(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Set up directory for output\n",
    "folder_path = os.path.join(folder_path, '{sub}')\n",
    "\n",
    "# Define output paths\n",
    "output_paths = {\n",
    "    'descriptives': folder_path.format(sub=f'descriptives'),\n",
    "    'evaluation': folder_path.format(sub=f'evaluation'),\n",
    "    'summary_tables':folder_path.format(sub=f'summary_tables'),\n",
    "    'predictions':folder_path.format(sub=f'predictions'),\n",
    "    'maps':folder_path.format(sub=f'maps'),\n",
    "    'models':folder_path.format(sub=f'models'),\n",
    "    'data':folder_path.format(sub=f'data'),\n",
    "\n",
    "    # Sub folders \n",
    "    'scores_tables': os.path.join(folder_path.format(sub=f\"evaluation\"), \"scores_tables\"),\n",
    "    'coord_plots': os.path.join(folder_path.format(sub=f\"evaluation\"), \"coord_plots\"),\n",
    "    'bootstrapped': os.path.join(folder_path.format(sub=f\"evaluation\"), \"bootstrapped\"),\n",
    "    'pr_curves': os.path.join(folder_path.format(sub=f\"evaluation\"), \"pr_curves\"),\n",
    "    'features': os.path.join(folder_path.format(sub=f\"evaluation\"), \"features\"),\n",
    "    'bisep': os.path.join(folder_path.format(sub=f\"evaluation\"), \"bisep\"),\n",
    "\n",
    "}\n",
    "\n",
    "# Create new folders if they do not already exist.\n",
    "for k, v in output_paths.items():\n",
    "    if not os.path.isdir(v):\n",
    "        os.makedirs(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749ab78-dcac-49ff-af01-5fd2b8275632",
   "metadata": {},
   "source": [
    "## Specifying global parameters <a class=\"anchor\" id=\"specify_parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c52f7-75f0-44b4-bc0d-e3dd05c9ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here the outcome of interest to run the analysis for onset and the new periods.\n",
    "# Possible ids to choose from: 'incidence' (main analysis), 'onset', 'incidence_np' (adjusted period), 'onset_np' (adjusted period)\n",
    "run_outcome = 'incidence' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7d7b8-8555-44f8-8613-b22a0eced89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the outcome variable\n",
    "depvar = 'ged_sb_dummy_dep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bad97a-11f8-4733-a67b-cf4a228800b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here whether to retrain the models from scratch or whether to use the pre-trained model objects used for the analysis. \n",
    "train = False\n",
    "evaluate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c4b7f-d11c-40d6-bdd5-7d4ed058432a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86abcad-f75f-4af0-88b2-e24bdae95a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "with open(os.path.join(output_paths['data'], f\"data_dict_incidence.p\"), 'rb') as fp:\n",
    "    datasets = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2dcd69-768a-4d69-b749-46d225b00684",
   "metadata": {},
   "source": [
    "## Define Regressors, Downsampling and Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ece947-9c56-40a7-b6d6-a0da6029c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random forest hyper-parameters.\n",
    "nj=14\n",
    "n_estimators=500\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_jobs=nj, n_estimators=n_estimators, random_state=1308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3e59e-f017-4d4a-9bd1-8709702533bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define downsampling strategy.\n",
    "downsampling = Downsampling(share_positive = 1.0, share_negative = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define steps. \n",
    "steps = [3,6,12,36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c11025-9811-4c33-bf3b-f7db12c59fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define periods. \n",
    "periods = [\n",
    "    Period(name=\"A\",train_start=205,train_end=408,predict_start=409,predict_end=444),\n",
    "    Period(name=\"B\",train_start=205,train_end=444,predict_start=445,predict_end=480),\n",
    "]\n",
    "\n",
    "adj_periods = [\n",
    "    Period(name=\"A\",train_start=205,train_end=408-36,predict_start=409-36,predict_end=444-36),\n",
    "    Period(name=\"B\",train_start=205,train_end=444-36,predict_start=445-36,predict_end=480-36),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71192cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which periods to train/test on. \n",
    "# Possible ids incldude 'periods', 'adj_periods'\n",
    "periods_model = periods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7626c-bb0c-47c9-b259-497bcd0b8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview.\n",
    "print('Steps', steps)\n",
    "print('Downsampling', downsampling)\n",
    "print('Periods', periods_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147b2ed-b4ed-4cd6-9d6d-ff82c99efbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model definitions. \n",
    "for df in datasets:\n",
    "    print(df['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739bf8a-b1f9-431f-a6e7-f1de11e08251",
   "metadata": {},
   "source": [
    "## Specify Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80987ad7-c778-47ad-b71e-10b12bae087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check parameters.\n",
    "print('Dependent variable:', depvar, ', Outcome:', run_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ca455-2a5b-48c7-a19d-7925b6fb5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to be trained. \n",
    "models_to_train = [\n",
    "    'baseline_simple',\n",
    "    'econ_nat_bl',\n",
    "    'econ_full_bl',\n",
    "    'inst_elecdemo_bl',\n",
    "    'inst_civlib_bl',\n",
    "    'inst_elect_bl',\n",
    "    'inst_devi_bl',\n",
    "    'pr_naive_bl',\n",
    "    'pr_dynamic_loc_bl',\n",
    "    'pr_dynamic_nat_bl',\n",
    "    'pr_elecdemo_bl',\n",
    "    'pr_civlib_bl',\n",
    "    'pr_elect_bl',\n",
    "    'pr_devi_bl',\n",
    "    'pr_econ_nat_bl',\n",
    "    'pr_econ_full_bl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b49c1f-bebe-42a9-9741-ab1fc329b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of models based on the previously defined parameters.\n",
    "ModelList = []\n",
    "\n",
    "with open('featlist_protest_paper.yaml', 'r') as file:\n",
    "    full_featlist = yaml.safe_load(file)\n",
    "    \n",
    "for df in datasets:\n",
    "    for mname in models_to_train:\n",
    "        if df['Name'] == mname:\n",
    "            print(mname)\n",
    "            \n",
    "            for feats,colname in zip(full_featlist.keys(),full_featlist.values()):\n",
    "                if df['Name'] == feats:\n",
    "                    featlist_dep = list(df['df'][colname].columns)\n",
    "                    featlist_f = [x for x in featlist_dep if x != f'{depvar}']\n",
    "            \n",
    "            if 'incidence' in run_outcome:\n",
    "                print('incidence')\n",
    "                ModelList.append(\n",
    "                    Model(\n",
    "                        name = f'protest_{mname}_{run_outcome}',\n",
    "                        col_outcome = depvar,\n",
    "                        cols_features = featlist_f,\n",
    "                        periods = periods_model,\n",
    "                        steps = steps,\n",
    "                        outcome_type = \"prob\",\n",
    "                        downsampling =  downsampling,\n",
    "                        estimator = rf_classifier,\n",
    "                        dir_storage = output_paths['models'],\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            if 'onset' in run_outcome:\n",
    "                print('onset')\n",
    "                ModelList.append(\n",
    "                    Model(\n",
    "                        name = f'protest_{mname}_{run_outcome}',\n",
    "                        col_outcome = depvar,\n",
    "                        cols_features = featlist_f,\n",
    "                        periods = periods_model,\n",
    "                        steps = steps,\n",
    "                        outcome_type = \"prob\",\n",
    "                        downsampling =  downsampling,\n",
    "                        onset_outcome= True,\n",
    "                        onset_window= 6, \n",
    "                        estimator = rf_classifier,\n",
    "                        dir_storage = output_paths['models'],\n",
    "                    )\n",
    "                )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b0308-8c60-4a31-b7c2-070270cfdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ModelList:\n",
    "    print(model.name)\n",
    "    print(model.col_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60444c10-1ff6-4010-a27e-728c58778dbe",
   "metadata": {},
   "source": [
    "## Train, Calibrate, Predict & Evaluate <a class=\"anchor\" id=\"trainpredict\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd16b92-423a-420d-a083-f580f384f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving or Loading - dependent on parameter at the top of the notebook.\n",
    "if train:\n",
    "    save_preds = True\n",
    "else:\n",
    "    save_preds = False\n",
    "    \n",
    "print(save_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6aed39-123a-43ac-bb9d-7da6bc149da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.\n",
    "if train:\n",
    "    random.seed(1308)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    for df in datasets:\n",
    "        for model in ModelList:\n",
    "            mname = df['Name']\n",
    "            model.name\n",
    "            if  model.name == f'protest_{mname}_{run_outcome}':\n",
    "                print(f'Fitting {model.name}')\n",
    "\n",
    "                print(datetime.now())\n",
    "                model.fit_estimators(df['df'])\n",
    "                print(datetime.now())\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb887b-3c9a-41ed-b497-18b8d673fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict.\n",
    "if train:\n",
    "    random.seed(1308)\n",
    "    preds_dict = {}\n",
    "    start_time = datetime.now()\n",
    "    for df in datasets:\n",
    "        for model in ModelList:\n",
    "            mname = df['Name']\n",
    "            if  model.name == f'protest_{mname}_{run_outcome}':\n",
    "                print(f'Predicting for {mname}')\n",
    "                print(datetime.now())\n",
    "                preds_dict[model.name] = assign_into_df(df_from=model.predict(df['df']), df_to=df['df']).loc[periods_model[1].predict_start:periods_model[1].predict_end]\n",
    "                print(datetime.now())\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266bc21-1244-486e-bd25-d77c926bf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate.\n",
    "if train:\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for df in datasets:\n",
    "        for model in ModelList:\n",
    "            mname = df['Name']\n",
    "            if  model.name == f'protest_{mname}_{run_outcome}':\n",
    "    \n",
    "                print(f'Evaluating {model.name}')\n",
    "\n",
    "                print(datetime.now())\n",
    "                model.evaluate(df['df'])\n",
    "                print(datetime.now())\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41021cdd-1ff8-4a74-a8e7-9d6a8cfd6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading previously trained models based on the above defined outcome. \n",
    "if save_preds:\n",
    "    for df in datasets:\n",
    "        for model in ModelList:\n",
    "            mname = df['Name']\n",
    "            if  model.name == f'protest_{mname}_{run_outcome}':\n",
    "                \n",
    "                # Save models\n",
    "                model.save(os.path.join(output_paths['models'],f'{mname}_{run_outcome}.joblib'))\n",
    "                \n",
    "    # Save dictonary.\n",
    "    with open(os.path.join(output_paths['predictions'], f\"preds_dict_{run_outcome}.p\"), 'wb') as fp:\n",
    "        pickle.dump(preds_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # Load models.\n",
    "    ModelList = []\n",
    "    models_to_load = models_to_train\n",
    "    \n",
    "    for m in models_to_load:\n",
    "        ModelList.append(Model.load(os.path.join(output_paths['models'],f'{m}_{run_outcome}.joblib')))\n",
    "    \n",
    "    # Load dictonary.\n",
    "    with open(os.path.join(output_paths['predictions'], f\"preds_dict_{run_outcome}.p\"), 'rb') as fp:\n",
    "        preds_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eea998-1243-43ce-b86f-7ebef4d7c0ec",
   "metadata": {},
   "source": [
    "## Evaluation <a class=\"anchor\" id=\"evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df332532-1d32-44f8-b703-d646ff21fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(evaltools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7faef6f-733e-42ea-9e0e-8899c3c7b72a",
   "metadata": {},
   "source": [
    "### Evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5bb26-96de-4cc9-b288-16470953ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ModelList:\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411fab4-fbea-49b0-bec9-1e867fd5f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print scores for quick overview.\n",
    "for model in ModelList:\n",
    "    print(model.name)\n",
    "    print(model.scores['B'][3]['uncalibrated']['average_precision'])\n",
    "    print(model.scores['B'][6]['uncalibrated']['average_precision'])\n",
    "    print(model.scores['B'][12]['uncalibrated']['average_precision'])\n",
    "    print(model.scores['B'][36]['uncalibrated']['average_precision'])\n",
    "    print(model.scores['B'][3]['uncalibrated']['area_under_roc'])\n",
    "    print(model.scores['B'][6]['uncalibrated']['area_under_roc'])\n",
    "    print(model.scores['B'][12]['uncalibrated']['area_under_roc'])\n",
    "    print(model.scores['B'][36]['uncalibrated']['area_under_roc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d007f8d-b657-4bd5-afe7-cf65a3e66762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the scores to one df and one table. \n",
    "dfs_scores = []\n",
    "for evalm in ['AP','AUROC','Brier']:\n",
    "    df_scores = evaltools.df_eval_scores(\n",
    "        preds_dict = preds_dict,\n",
    "        model_list = models_to_train, \n",
    "        run_outcome = run_outcome,\n",
    "        ev_name = evalm,\n",
    "        depvar = depvar,\n",
    "        steps = steps,\n",
    "        round_to = 3, \n",
    "        path= os.path.join(output_paths['scores_tables'], f\"eval_{evalm}_{run_outcome}.tex\"\n",
    "                          )\n",
    "    )\n",
    "    # Store in list.\n",
    "    dfs_scores.append(df_scores)\n",
    "    \n",
    "# Rename\n",
    "rename_models_dict = {\n",
    "    f'baseline_simple':'M0',\n",
    "    f'econ_nat_bl': 'M8 w/o pr',\n",
    "    f'econ_full_bl': 'M9 w/o pr',\n",
    "    f'inst_elecdemo_bl': 'M4 w/o pr',\n",
    "    f'inst_civlib_bl' : 'M5 w/o pr',\n",
    "    f'inst_elect_bl' : 'M6 w/o pr',\n",
    "    f'inst_devi_bl': 'M7 w/o pr',\n",
    "    f'inst_election_econ_national_bl{run_outcome}': 'M6M8 w/o pr',\n",
    "    #f'polinst_election_econ_full_bl': 'M6M9 w/o pr',\n",
    "    #f'polinst_devi_econ_national_bl': 'M7M8 w/o pr',\n",
    "    #f'polinst_devi_econ_full_bl': 'M7M9 w/o pr',\n",
    "    f'pr_naive_bl': 'M1',\n",
    "    f'pr_dynamic_loc_bl': 'M2',\n",
    "    f'pr_dynamic_nat_bl': 'M3',\n",
    "    f'pr_elecdemo_bl': 'M4',\n",
    "    f'pr_civlib_bl': 'M5',\n",
    "    f'pr_elect_bl': 'M6',\n",
    "    f'pr_devi_bl': 'M7',\n",
    "    f'pr_econ_nat_bl': 'M8',\n",
    "    f'pr_econ_full_bl': 'M9',\n",
    "    #f'pr_polinst_election_econ_national_bl_{run_outcome}': 'M6M8',\n",
    "    #f'pr_polinst_election_econ_full_bl{run_outcome}': 'M6M9',\n",
    "    #f'pr_polinst_devi_econ_national_bl{run_outcome}': 'M7M8',\n",
    "    #f'pr_polinst_devi_econ_full_bl{run_outcome}': 'M7M9'\n",
    "}\n",
    "reorder_models = [\n",
    "    'M0',\n",
    "    #'M0',\n",
    "    'M1',\n",
    "    'M2',\n",
    "    'M3',\n",
    "    'M4',\n",
    "    'M4 w/o pr',\n",
    "    'M5',\n",
    "    'M5 w/o pr',\n",
    "    'M6',\n",
    "    'M6 w/o pr',\n",
    "    'M7',\n",
    "    'M7 w/o pr',\n",
    "    'M8',\n",
    "    'M8 w/o pr',\n",
    "    'M9',\n",
    "    'M9 w/o pr',\n",
    "    'M6M8',\n",
    "    'M6M8 w/o pr',\n",
    "    'M6M9',\n",
    "    'M6M9 w/o pr',\n",
    "    'M7M8',\n",
    "    'M7M8 w/o pr',\n",
    "    'M7M9',\n",
    "    'M7M9 w/o pr',\n",
    "]\n",
    "\n",
    "# Make into single df.\n",
    "dfs_scores_all = pd.concat(dfs_scores,axis=1)\n",
    "dfs_scores_all = dfs_scores_all.rename(index=rename_models_dict)\n",
    "dfs_scores_all = dfs_scores_all.reindex(reorder_models).dropna()\n",
    "dfs_scores_all.to_csv(os.path.join(output_paths['scores_tables'], f\"eval_all_{run_outcome}.csv\"))\n",
    "print(dfs_scores_all)\n",
    "\n",
    "# Write to tex. file. \n",
    "path = os.path.join(output_paths['scores_tables'], f\"eval_all_{run_outcome}.tex\")\n",
    "tex = dfs_scores_all.reset_index().to_latex(index=False)\n",
    "\n",
    "# Get meta infromation\n",
    "now = datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "meta = f\"\"\"\n",
    "%Date: {now}\n",
    "%Output created by protest_paper.ipynb.\n",
    "%Compare eval metrics for all models.\n",
    "\\\\\n",
    "\"\"\"\n",
    "tex = meta + tex\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(tex)\n",
    "print(f\"Wrote scores table to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e5d8f-9856-43a4-a9c1-1e0689950377",
   "metadata": {},
   "source": [
    "### Parallel coordinate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03352b2c-5c45-4c7c-8bfb-2c999b479cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make coordinate plots\n",
    "dfs_coords = pd.read_csv(os.path.join(output_paths['scores_tables'], f\"eval_all_{run_outcome}.csv\"),index_col=[0],header=[0,1], skipinitialspace=True)\n",
    "dfs_coords_copy = dfs_coords.swaplevel(axis=1)\n",
    "dfs_coords_copy = dfs_coords_copy.reindex([('3', 'AP'),\n",
    " ('3', 'AUROC'),\n",
    " ('3', 'Brier'),\n",
    " ('6', 'AP'),\n",
    " ('6', 'AUROC'),\n",
    " ('6', 'Brier'),\n",
    " ('12', 'AP'),\n",
    " ('12', 'AUROC'),\n",
    " ('12', 'Brier'),\n",
    " ('36', 'AP'),\n",
    " ('36', 'AUROC'),\n",
    " ('36', 'Brier')], axis=1)\n",
    "dfs_coords_copy = dfs_coords_copy[dfs_coords_copy.index.isin(['M0','M1','M2'])]\n",
    "\n",
    "evaltools.plot_parcoord_allsteps(\n",
    "    df = dfs_coords_copy,\n",
    "    steps = ['3','6','12','36'],\n",
    "    reverse = True,\n",
    "    cmap='Dark2',\n",
    "    legend_label=['M0','M1','M2',],\n",
    "    path = os.path.join(output_paths[\"coord_plots\"], f\"coord_simple_{run_outcome}.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae95b5-a8a8-41c4-b4e8-bb3be8da32d2",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b7db6-0a1d-482f-a95e-5f1b0e61c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_bs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7414ba-cf2c-43e2-b221-e14f23a28375",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_bs:\n",
    "    # Make list of all model names.\n",
    "    all_model_names = []\n",
    "    for model in ModelList:\n",
    "        all_model_names.append(model.name)\n",
    "\n",
    "    # Select models.\n",
    "    coord_models_names = []\n",
    "    matchers = [\n",
    "        f'protest_baseline_simple_{run_outcome}',\n",
    "        f'protest_pr_naive_bl_{run_outcome}',\n",
    "        f'protest_pr_dynamic_loc_bl_{run_outcome}',\n",
    "        f'protest_pr_dynamic_nat_bl_{run_outcome}',\n",
    "        f'protest_inst_elecdemo_bl_{run_outcome}',\n",
    "        f'protest_pr_elecdemo_bl_{run_outcome}',\n",
    "        f'protest_inst_civlib_bl_{run_outcome}',\n",
    "        f'protest_pr_civlib_bl_{run_outcome}',\n",
    "        f'protest_inst_elect_bl_{run_outcome}',\n",
    "        f'protest_pr_elect_bl_{run_outcome}',\n",
    "        f'protest_inst_devi_bl_{run_outcome}',\n",
    "        f'protest_pr_devi_bl_{run_outcome}',\n",
    "        f'protest_econ_nat_bl_{run_outcome}',\n",
    "        f'protest_pr_econ_nat_bl_{run_outcome}',\n",
    "        f'protest_econ_full_bl_{run_outcome}',\n",
    "        f'protest_pr_econ_full_bl_{run_outcome}'\n",
    "    ]\n",
    "\n",
    "    coord_models_names = [s for s in all_model_names if any(xs in s for xs in matchers)]\n",
    "\n",
    "    models_to_boot = []\n",
    "    for model in ModelList:\n",
    "        if model.name in coord_models_names:\n",
    "            models_to_boot.append(model.name)\n",
    "\n",
    "\n",
    "    eval_fun = 'average_precision'\n",
    "    steps = steps\n",
    "\n",
    "    dfs_boots=[]\n",
    "    for model_to_boot in models_to_boot:\n",
    "        print(model_to_boot)\n",
    "        for step in steps:\n",
    "            print(step)\n",
    "            df_boots = evaltools.boot_evalmetric(\n",
    "                model_name = model_to_boot,\n",
    "                preds_dict = preds_dict, \n",
    "                depvar = depvar,\n",
    "                step=step,\n",
    "                eval_fun = eval_fun,\n",
    "                set_seed = 1308,\n",
    "                n_bootstraps=1000,\n",
    "            )\n",
    "\n",
    "            dfs_boots.append(df_boots)\n",
    "\n",
    "    df_boots_all = pd.concat(dfs_boots,axis=1) \n",
    "    df_boots_all.to_csv(os.path.join(output_paths[\"bootstrapped\"], f\"boots_ap_{run_outcome}.csv\"))\n",
    "    print('df written to', f\"boots_ap_{run_outcome}.csv\")\n",
    "    \n",
    "else:\n",
    "    df_boots_all = pd.read_csv(os.path.join(output_paths[\"bootstrapped\"], f\"boots_ap_{run_outcome}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb695d-845e-4c3d-bf09-f48dfd6bdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters.\n",
    "\n",
    "titles = [\n",
    "    'M1 vs M0',\n",
    "    'M2 vs M1',\n",
    "    'M3 vs M1',\n",
    "    'M2 vs M0',\n",
    "    'M3 vs M0',\n",
    "    'M4 vs M2',\n",
    "    'M5 vs M2',\n",
    "    'M6 vs M2',\n",
    "    'M7 vs M2',\n",
    "    'M8 vs M2',\n",
    "    'M9 vs M2',\n",
    "    'M4 vs M4 w/o pr ',\n",
    "    'M5 vs M5 w/o pr',\n",
    "    'M6 vs M6 w/o pr',\n",
    "    'M7 vs M7 w/o pr',\n",
    "    'M8 vs M8 w/o pr',\n",
    "    'M9 vs M9 w/o pr',\n",
    "\n",
    "]\n",
    "\n",
    "model1 =[\n",
    "    f'protest_pr_naive_bl_{run_outcome}_average_precision_', #M1\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', #M2\n",
    "    f'protest_pr_dynamic_nat_bl_{run_outcome}_average_precision_', # M3\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', #M2\n",
    "    f'protest_pr_dynamic_nat_bl_{run_outcome}_average_precision_', # M3\n",
    "    f'protest_pr_elecdemo_bl_{run_outcome}_average_precision_', #M4\n",
    "    f'protest_pr_civlib_bl_{run_outcome}_average_precision_', #M5\n",
    "    f'protest_pr_elect_bl_{run_outcome}_average_precision_', #M6\n",
    "    f'protest_pr_devi_bl_{run_outcome}_average_precision_', #M7\n",
    "    f'protest_pr_econ_nat_bl_{run_outcome}_average_precision_', #M8\n",
    "    f'protest_pr_econ_full_bl_{run_outcome}_average_precision_', #M9\n",
    "    f'protest_pr_elecdemo_bl_{run_outcome}_average_precision_', #M4\n",
    "    f'protest_pr_civlib_bl_{run_outcome}_average_precision_', #M5\n",
    "    f'protest_pr_elect_bl_{run_outcome}_average_precision_', #M6\n",
    "    f'protest_pr_devi_bl_{run_outcome}_average_precision_', #M7\n",
    "    f'protest_pr_econ_nat_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_pr_econ_full_bl_{run_outcome}_average_precision_',\n",
    "    \n",
    "\n",
    "]\n",
    "\n",
    "model2 = [\n",
    "    f'protest_baseline_simple_{run_outcome}_average_precision_', # M0\n",
    "    f'protest_pr_naive_bl_{run_outcome}_average_precision_', #M1\n",
    "    f'protest_pr_naive_bl_{run_outcome}_average_precision_', #M1\n",
    "    f'protest_baseline_simple_{run_outcome}_average_precision_', # M0\n",
    "    f'protest_baseline_simple_{run_outcome}_average_precision_', # M0\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_pr_dynamic_loc_bl_{run_outcome}_average_precision_', # M2\n",
    "    f'protest_inst_elecdemo_bl_{run_outcome}_average_precision_', #M4 wo pr\n",
    "    f'protest_inst_civlib_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_inst_elect_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_inst_devi_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_econ_nat_bl_{run_outcome}_average_precision_',\n",
    "    f'protest_econ_full_bl_{run_outcome}_average_precision_',  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9e012-9ccb-474b-85ca-fa0b16ecdbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters.\n",
    "legendtrue = False\n",
    "\n",
    "# Adjust these parameters for different outcomes \n",
    "if run_outcome == 'incidence':\n",
    "    ymin = -0.02 \n",
    "    ymax = 0.12\n",
    "\n",
    "# Onset\n",
    "if run_outcome == 'onset':\n",
    "    ymin = -0.1 # Adjust these parameters for different outcomes \n",
    "    ymax = 0.14 # Adjust these parameters for different outcomes \n",
    "\n",
    "evaltools.plot_bootstrapped_diff(\n",
    "    df=df_boots_all,\n",
    "    titles=titles,\n",
    "    modellist1=model1,\n",
    "    modelllist2=model2,\n",
    "    legendtrue=legendtrue,\n",
    "    steps=steps,\n",
    "    ymin=ymin,\n",
    "    ymax=ymax,\n",
    "    save_as=run_outcome,\n",
    "    path_out=output_paths[\"bootstrapped\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c4ef0-5927-4a64-a734-7a12d08bbbab",
   "metadata": {},
   "source": [
    "### Prediction maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd492222-3626-4b88-99b1-d9c7923bc585",
   "metadata": {},
   "source": [
    "#### Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de081bdb-6efc-4b8b-8b39-2779df508e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch geometries from database\n",
    "fetch_and_save_gdf = False\n",
    "if fetch_and_save_gdf:\n",
    "    gdf_mapping = organize.fetch_gdf()\n",
    "    gdf_mapping = gdf_mapping.loc[445:480]\n",
    "    gdf_mapping.to_csv(os.path.join(output_paths[\"data\"], f\"gdf_mapping.csv\"))    \n",
    "else:\n",
    "    gdf_mapping = pd.read_csv(os.path.join(output_paths[\"data\"], f\"gdf_mapping.csv\")).set_index(['month_id','priogrid_gid'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044f706-c216-4eaa-8a1c-ca9d3bad9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "#import sqlalchemy as sa\n",
    "#from ingester3.config import source_db_path\n",
    "\n",
    "fetch_and_save_gdf = False\n",
    "if fetch_and_save_gdf:\n",
    "\n",
    "    engine = sa.create_engine(source_db_path)\n",
    "\n",
    "    gdf_ci_master = gpd.GeoDataFrame.from_postgis(\n",
    "        \"SELECT id as country_id, in_africa, in_me, geom FROM prod.country\",\n",
    "        engine,\n",
    "        geom_col='geom'\n",
    "    )\n",
    "    gdf_ci_master = gdf_ci_master.to_crs(4326)\n",
    "    gdf_ci_master.to_csv(os.path.join(output_paths[\"data\"], f\"gdf_mapping_ci_master.csv\")) \n",
    "else:\n",
    "    gdf_ci_master = pd.read_csv(os.path.join(output_paths[\"data\"], f\"gdf_mapping_ci_master.csv\"))\n",
    "    gdf_ci_master['geom'] = gdf_ci_master['geom'].apply(wkt.loads)\n",
    "    gdf_ci_master = gpd.GeoDataFrame(gdf_ci_master, crs=\"EPSG:4326\", geometry='geom')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8431238-f238-4b97-8366-23257e7ca0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match steps with months.\n",
    "times = [447,450,456,480] \n",
    "allsteps = steps\n",
    "times_steps = dict(zip(times, allsteps)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6d854-b626-4cf5-a8f8-eb73641bce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scale\n",
    "proba_dict= {'0.1%':0.001, #'0.2%':0.002, '0.5%': 0.005,\n",
    "               #'1%':0.01, \n",
    "             '2%':0.02, '5%': 0.05,\n",
    "               '10%':0.1, '20%':0.2, '40%': 0.4,\n",
    "               '60%':0.6, '80%':0.8, '90%': 0.9,\n",
    "               '95%':0.95, '99%':0.99, \n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e2909-a4f3-4186-90e2-2629f1b35a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_to_train:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ca27f-a9cf-4a35-b795-5a699dae6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full map.\n",
    "\n",
    "for model in [models_to_train[0]]:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(mname)\n",
    "    mname_plt = model.replace('simple_','')\n",
    "    print(mname_plt)\n",
    "\n",
    "    for key, value in times_steps.items():\n",
    "        \n",
    "        mapdf = pd.concat([preds_dict[mname][f'ss_{mname}_{value}'],preds_dict[mname][depvar],gdf_mappingt],axis=1)  \n",
    "        mapdf['geometry'] = mapdf['geometry'].apply(wkt.loads)\n",
    "        mapdf = gpd.GeoDataFrame(mapdf, geometry=\"geometry\")\n",
    "        print(key,value)\n",
    "        \n",
    "        pgm_masked=Mapper2(\n",
    "            width=20,\n",
    "            height=20,\n",
    "            frame_on=True,\n",
    "            bbox=bbox_from_cid_region('africa'),\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key],\n",
    "            map_dictionary = proba_dict,\n",
    "            cmap = 'rainbow',\n",
    "            transparency = 1,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            column=f'ss_{mname}_{value}',\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key][mapdf.loc[key][depvar]==1].geometry.centroid,\n",
    "            marker=\"o\",\n",
    "            markersize=5,\n",
    "            color=\"black\"\n",
    "        ).add_views_textbox(\n",
    "            text=f'Run: protest \\nModel: {model}, \\nMonth: {key} (Step {value})',\n",
    "            textsize=20)\n",
    "        \n",
    "        ax = pgm_masked.ax\n",
    "        gdf_ci_master.plot(ax=ax,edgecolor='grey',linewidth=0.2,facecolor='None')\n",
    "\n",
    "        pgm_masked.save(output_paths['maps']+f'/baseline_step{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90153c60-2ce4-48f7-a9eb-3057e14c7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in on specific area.\n",
    "\n",
    "for model in [models_to_train[0]]:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(mname)\n",
    "    mname_plt = model.replace('simple_','')\n",
    "    print(mname_plt)\n",
    "\n",
    "    for key, value in times_steps.items():\n",
    "        \n",
    "        mapdf = pd.concat([preds_dict[mname][f'ss_{mname}_{value}'],preds_dict[mname][depvar],gdf_mapping],axis=1)  \n",
    "        mapdf['geometry'] = mapdf['geometry'].apply(wkt.loads)\n",
    "        mapdf = gpd.GeoDataFrame(mapdf, geometry=\"geometry\")\n",
    "        print(key,value)\n",
    "        \n",
    "        pgm_masked=Mapper2(\n",
    "            width=20,\n",
    "            height=20,\n",
    "            frame_on=True,\n",
    "            bbox=[22.6716, 48.819, -2.8909, 16.2484],\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key],\n",
    "            map_dictionary = proba_dict,\n",
    "            cmap = 'rainbow',\n",
    "            transparency = 1,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "            column=f'ss_{mname}_{value}',\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key][mapdf.loc[key][depvar]==1].geometry.centroid,\n",
    "            marker=\"o\",\n",
    "            markersize=5,\n",
    "            color=\"black\"\n",
    "        ).add_views_textbox(\n",
    "            text=f'Run: protest \\nModel: {model}, \\nMonth: {key} (Step {value})',\n",
    "            textsize=20)\n",
    "        \n",
    "        ax = pgm_masked.ax\n",
    "        gdf_ci_master.plot(ax=ax,edgecolor='grey',linewidth=0.2)\n",
    "\n",
    "        pgm_masked.save(output_paths['maps']+f'/baseline_step{value}_zoom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d7ac1-d4e3-4e2e-ad6b-6cf8d5641d22",
   "metadata": {},
   "source": [
    "#### Map differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a48dc-336d-4803-b99e-c60546e3d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the baseline model (to compute differnce)\n",
    "baseline = True\n",
    "baselinename = f'protest_baseline_simple_{run_outcome}'\n",
    "\n",
    "for model in models_to_train:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(model)\n",
    "    \n",
    "    for s in steps:\n",
    "        preds_dict[mname][f'diff_{mname}_{s}'] =  preds_dict[mname][f'ss_{mname}_{s}']-preds_dict[baselinename][f'ss_{baselinename}_{s}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2a27d-5080-4bda-b544-ef8359fcacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scale.\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "proba_dict= {'-90%':-0.9, \n",
    "             '-60%':-0.6,\n",
    "             '-40%':-0.4,\n",
    "             '-20%':-0.2,\n",
    "             '0%':0,\n",
    "             '20%':0.2,\n",
    "             '40%':0.4,\n",
    "             '60%':0.6,\n",
    "             '90%':0.9,\n",
    "              }\n",
    "cmap = plt.get_cmap('seismic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e0dd4-95c5-4ec6-ae27-a5b034707556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full maps.\n",
    "for model in models_to_train:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(mname)\n",
    "    mname_plt = model.replace('simple_','')\n",
    "    print(mname_plt)\n",
    "\n",
    "    for key, value in times_steps.items():\n",
    "        \n",
    "        mapdf = pd.concat([preds_dict[mname][f'diff_{mname}_{s}'],preds_dict[mname][depvar],gdf_mapping],axis=1)  \n",
    "        mapdf['geometry'] = mapdf['geometry'].apply(wkt.loads)\n",
    "        mapdf = gpd.GeoDataFrame(mapdf, geometry=\"geometry\")\n",
    "        print(key,value)\n",
    "        \n",
    "        pgm_masked=Mapper2(\n",
    "            width=20,\n",
    "            height=20,\n",
    "            frame_on=True,\n",
    "            bbox=bbox_from_cid_region('africa'),\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key],\n",
    "            map_dictionary = proba_dict,\n",
    "            cmap = cmap,\n",
    "            transparency = 1,\n",
    "            linewidth=0.5,\n",
    "            column=f'diff_{mname}_{s}',\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key][mapdf.loc[key][depvar]==1].geometry.centroid,\n",
    "            marker=\"o\",\n",
    "            markersize=5,\n",
    "            #column=f'actuals_step{value}',\n",
    "            color=\"black\"\n",
    "        ).add_views_textbox(\n",
    "            text=f'Run: protest \\nModel: {model}, \\nMonth: {key} (Step {value})',\n",
    "            textsize=20)\n",
    "        \n",
    "        ax = pgm_masked.ax\n",
    "        gdf_ci_master.plot(ax=ax,edgecolor='grey',linewidth=0.2,facecolor='None')\n",
    "\n",
    "        pgm_masked.save(output_paths['maps']+f'/diff_{model}_step{value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5e6cb-e27a-4bfd-befa-3967fb8a922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in on specific area.\n",
    "\n",
    "for model in models_to_train:\n",
    "    \n",
    "    mname = f'protest_{model}_{run_outcome}'\n",
    "    print(mname)\n",
    "    mname_plt = model.replace('simple_','')\n",
    "    print(mname_plt)\n",
    "\n",
    "    for key, value in times_steps.items():\n",
    "        \n",
    "        mapdf = pd.concat([preds_dict[mname][f'diff_{mname}_{s}'],preds_dict[mname][depvar],gdf_mapping],axis=1)  \n",
    "        mapdf['geometry'] = mapdf['geometry'].apply(wkt.loads)\n",
    "        mapdf = gpd.GeoDataFrame(mapdf, geometry=\"geometry\")\n",
    "        print(key,value)\n",
    "        \n",
    "        pgm_masked=Mapper2(\n",
    "            width=20,\n",
    "            height=20,\n",
    "            frame_on=True,\n",
    "            bbox=[22.6716, 48.819, -2.8909, 16.2484],\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key],\n",
    "            map_dictionary = proba_dict,\n",
    "            cmap = cmap,\n",
    "            transparency = 1,\n",
    "            linewidth=0.5,\n",
    "            column=f'diff_{mname}_{s}',\n",
    "        ).add_layer(\n",
    "            gdf=mapdf.loc[key][mapdf.loc[key][depvar]==1].geometry.centroid,\n",
    "            marker=\"o\",\n",
    "            markersize=5,\n",
    "            #column=f'actuals_step{value}',\n",
    "            color=\"black\"\n",
    "        ).add_views_textbox(\n",
    "            text=f'Run: protest \\nModel: {model}, \\nMonth: {key} (Step {value})',\n",
    "            textsize=20)\n",
    "        \n",
    "        ax = pgm_masked.ax\n",
    "        gdf_ci_master.plot(ax=ax,edgecolor='grey',linewidth=0.2,facecolor='None')\n",
    "\n",
    "        pgm_masked.save(output_paths['maps']+f'/diff_{model}_step{value}_zoom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a39a3-fdef-4202-a04f-f389587840fd",
   "metadata": {},
   "source": [
    "### ICE/PDP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337e498-2f7d-4ae7-b313-b81c2631df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in datasets:\n",
    "    print(df['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f030a25-16a5-4a7d-bdac-b00d038fd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters \n",
    "m = ModelList[14] # Confirm that this is the full pr_econ model.\n",
    "pd_df = datasets[18]['df']\n",
    "partition='B'\n",
    "step = 3\n",
    "featlist = [\n",
    "    'decay_ts_6_acled_prex_dummy',\n",
    "    'decay_ts_6_acled_prin_dummy',\n",
    "    'decay_ts_6_acled_prpe_dummy',\n",
    "    'decay_ts_6_acled_prri_dummy'\n",
    "]\n",
    "\n",
    "# Plot.\n",
    "modelname = 'M9'\n",
    "sample_n = 1000\n",
    "\n",
    "# Change paramters here. For new estimation set \"create_save_pickle = True\"\n",
    "save_fig = True\n",
    "create_save_pickle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc8475-5f76-4e50-bd48-eb39907b3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pickle\n",
    "if create_save_pickle:\n",
    "    for feat in featlist:\n",
    "        print(feat)\n",
    "        pd_output = partial_dependence(\n",
    "            estimator = m.estimators.get(period_name=partition, step=step), \n",
    "            X = pd_df.loc[periods[1].train_start:periods[1].train_end][m.cols_features],\n",
    "            features=feat, \n",
    "            response_method='auto', \n",
    "            percentiles=(0, 1), \n",
    "            grid_resolution=20,  \n",
    "            kind='both',\n",
    "        )\n",
    "\n",
    "\n",
    "        pd_outputdict = dict(pd_output)\n",
    "        a_file = open(output_paths['features'] + f\"/pdp_{modelname}_s3_{feat}.pkl\", \"wb\")\n",
    "        pickle.dump(pd_outputdict, a_file)\n",
    "        a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96552f9-9847-40d3-bd89-749890716bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read pickle\n",
    "featlist = [\n",
    "    'decay_ts_6_acled_prex_dummy',\n",
    "    'decay_ts_6_acled_prin_dummy',\n",
    "    'decay_ts_6_acled_prpe_dummy',\n",
    "    'decay_ts_6_acled_prri_dummy'\n",
    "]\n",
    "\n",
    "modelname = 'M9'\n",
    "pd_df = datasets[18]['df']\n",
    "sample_n = 1000\n",
    "step = 3\n",
    "\n",
    "save_fig = True\n",
    "\n",
    "for feat in featlist:\n",
    "    pd_output = pd.read_pickle(output_paths['features'] + f\"/pdp_{modelname}_s3_{feat}.pkl\")\n",
    "    \n",
    "    print('Making Plot.')\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    pdp_sample = pd.DataFrame(pd_output['individual'][0]).sample(n=sample_n,replace=True)\n",
    "    pdp_sample = pdp_sample.apply(lambda row: row-pdp_sample.iloc[:, 0])\n",
    "    plt.plot(pdp_sample.T,color='lightgrey',linewidth=0.5)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "    xvals_cent = []\n",
    "    for i in pd_output['average'][0]:\n",
    "        xvals_cent.append(i-pd_output['average'][0][0])\n",
    "\n",
    "    # Add average.\n",
    "    plt.plot(pd_output['values'][0],xvals_cent,color='black')\n",
    "\n",
    "    # Add horizontal line. \n",
    "    ax.axhline(y=0, color='dimgrey', linestyle='--', lw=2)\n",
    "\n",
    "    # Add rug plot.\n",
    "    y_min, y_max = (-0.15, 0.15)\n",
    "    ax.plot(pd_df.loc[periods[1].train_start:periods[1].train_end][feat], [y_min]*len(pd_df.loc[periods[1].train_start:periods[1].train_end][feat]), '|', color='black',lw=0.5)\n",
    "\n",
    "    ax.set_ylim(-0.15, 0.15)\n",
    "\n",
    "    # Add title.\n",
    "    plt.title(f'Centered ICE plot for {feat}\\n {modelname}, step={step},\\n sample size={sample_n}, grid points={len(pdp_sample.columns)}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        fig.savefig(output_paths['features'] + f\"/{modelname}_s3_{feat}_adj.png\",\n",
    "                    dpi=200,\n",
    "                    facecolor=\"white\",\n",
    "                    bbox_inches=\"tight\",\n",
    "                )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd3c2b3-c7bc-4991-9a72-5ab55cb175af",
   "metadata": {},
   "source": [
    "### PR-Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fbfef-b529-4936-8685-8ff1ac8fe75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.cm.get_cmap('Dark2')\n",
    "colors = cm.colors\n",
    "step = 3\n",
    "fig_scale = 1\n",
    "\n",
    "\n",
    "# Dictonary with model comparisons to be plotted.\n",
    "dictoplots = [\n",
    "    {\n",
    "        'baseline_simple':'M0',\n",
    "        'pr_naive_bl':'M1'\n",
    "    },\n",
    "    {\n",
    "        'pr_naive_bl':'M1',\n",
    "        'pr_dynamic_loc_bl':'M2'\n",
    "    },\n",
    "    {\n",
    "        'pr_naive_bl':'M1',\n",
    "        'pr_dynamic_nat_bl':'M3'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_elecdemo_bl':'M4',\n",
    "        'inst_elecdemo_bl':'M4 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_civlib_bl':'M5',\n",
    "        'inst_civlib_bl':'M5 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_elect_bl':'M6',\n",
    "        'inst_elect_bl':'M6 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_devi_bl':'M7',\n",
    "        'inst_devi_bl':'M7 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_econ_nat_bl':'M8',\n",
    "        'econ_nat_bl':'M8 w/o pr'\n",
    "    },\n",
    "    {\n",
    "        'pr_dynamic_loc_bl':'M2',\n",
    "        'pr_econ_full_bl':'M9',\n",
    "        'econ_full_bl':'M9 w/o pr'\n",
    "    },\n",
    "]\n",
    "    \n",
    "for dic in dictoplots:\n",
    "\n",
    "    namelist = []\n",
    "    for i in dic.values():\n",
    "        namelist.append(i)\n",
    "    namelist = ''.join(namelist).replace('/','').replace(' ', '')\n",
    "\n",
    "    # Figure\n",
    "    fig = plt.figure(figsize=(8 * fig_scale, 8 * fig_scale))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    for model,clr, mname in zip(\n",
    "        dic.keys(),colors, dic.values()):\n",
    "        # Compute fpr, tpr, thresholds\n",
    "        precision, recall, _ = precision_recall_curve(preds_dict[f'protest_{model}_{run_outcome}'][depvar], preds_dict[f'protest_{model}_{run_outcome}'][f'ss_protest_{model}_{run_outcome}_{step}'])\n",
    "\n",
    "        #Plot\n",
    "        plt.plot(recall,precision,label=f'{mname}',color=clr)\n",
    "        plt.legend(title=\"Models\")\n",
    "        #plt.xlim([0.0, 1.0])\n",
    "        #plt.ylim([0.0, 1.02])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        \n",
    "        plt.savefig(\n",
    "            output_paths['pr_curves'] + f\"/pr_curve_{namelist}_s{step}.png\", bbox_inches='tight',dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eaeb38-9160-438a-8270-665d6e58402e",
   "metadata": {},
   "source": [
    "### Bi-separation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d79ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gdf.\n",
    "gdf_mapping = pd.read_csv(os.path.join(output_paths[\"data\"], f\"gdf_mapping.csv\"))\n",
    "gdf_mapping = gdf_mapping.rename(columns={'priogrid_gid':'pg_id'}).set_index(['month_id','pg_id']) \n",
    "\n",
    "gdf_filtered = df_filtered.set_index(['month_id','pg_id']).join(gdf_mapping,how='left')\n",
    "gdf_filtered['geometry'] = gdf_filtered['geometry'].astype(str)\n",
    "gdf_filtered['geometry'] = gdf_filtered['geometry'].apply(wkt.loads)\n",
    "gdf_filtered = gpd.GeoDataFrame(gdf_filtered, geometry=\"geometry\")\n",
    "gdf_filtered['lon'] = gdf_filtered['geometry'].centroid.x\n",
    "gdf_filtered['lat'] = gdf_filtered['geometry'].centroid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e19eb8-bf90-4fe8-b8e9-d767f2682ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_s = [445,454,463,472]\n",
    "time_e = [453,462,471,480]\n",
    "steps_sp= 3\n",
    "models1 = ['baseline_simple','pr_naive_bl','pr_naive_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl','pr_dynamic_loc_bl']\n",
    "models2 = ['pr_naive_bl','pr_dynamic_loc_bl','pr_dynamic_nat_bl','pr_elecdemo_bl','pr_civlib_bl','pr_elect_bl','pr_devi_bl','pr_econ_nat_bl','pr_econ_full_bl']\n",
    "models1_names = ['M0','M1','M1','M2','M2','M2','M2','M2','M2']\n",
    "models2_names = ['M1','M2','M3','M4','M5','M6','M7','M8','M9']\n",
    "\n",
    "df_filtered = preds_dict['protest_baseline_simple_incidence'].loc[450:452].reset_index().rename(columns={'priogrid_gid':'pg_id'})\n",
    "filtered_bbox = gdf_filtered[(gdf_filtered.lat>-4.3) & (gdf_filtered.lat<16.4) & (gdf_filtered.lon>22) & (gdf_filtered.lon<50)]\n",
    "filtered_bbox = filtered_bbox.reset_index()\n",
    "\n",
    "for m1,m2,m1names,m2names in zip(models1,models2,models1_names,models2_names):\n",
    "    for step in [steps_sp]:\n",
    "\n",
    "        dfm1,dfm2 = preds_dict[f'protest_{m1}_{run_outcome}'].reset_index(),preds_dict[f'protest_{m2}_{run_outcome}'].reset_index()\n",
    "        df_sp_1 = dfm1[dfm1['priogrid_gid'].isin(filtered_bbox.pg_id.unique())].set_index(['month_id','priogrid_gid']).drop(depvar,axis=1)\n",
    "        df_sp_2 = dfm2[dfm2['priogrid_gid'].isin(filtered_bbox.pg_id.unique())].set_index(['month_id','priogrid_gid'])\n",
    "\n",
    "        df_sp = pd.concat([df_sp_1,df_sp_2],axis=1)\n",
    "        print('Plot')\n",
    "        spl.BiseparationPlot(\n",
    "            df = df_sp.loc[447],\n",
    "            x = f'ss_protest_{m1}_{run_outcome}_{step}', \n",
    "            y = f'ss_protest_{m2}_{run_outcome}_{step}', \n",
    "            obs = f'{depvar}',\n",
    "            lab = 'priogrid_gid',\n",
    "            markersize=50,\n",
    "            title = f\"{m1names} versus {m2names}, step {step}\",\n",
    "            path = output_paths['bisep'] + f\"/sp_{m1names}_{m2names}_s{step}_447.png\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
